# Optimized Alert Rules Configuration for OBS-O2
# Designed to achieve <5% false positive rate with 0% false negative rate
# Version: 1.0.0
# Last Updated: 2025-09-14

global:
  # Global settings for all alerts
  evaluation_interval: 30s  # How often to evaluate rules
  alert_latency_target: 30s  # Target latency for alert generation
  cpu_overhead_target: 0.02  # Max 2% CPU overhead

  # Alert suppression defaults
  suppression:
    enabled: true
    deduplication_window: 300s  # 5 minutes
    flapping_detection: true
    flapping_window: 600s  # 10 minutes
    flapping_threshold: 3  # Number of state changes to consider flapping

# Dynamic thresholds based on time patterns
time_based_profiles:
  business_hours:
    schedule: "Mon-Fri 09:00-18:00 UTC"
    multiplier: 1.0  # Normal thresholds

  peak_hours:
    schedule: "Mon-Fri 12:00-14:00,18:00-20:00 UTC"
    multiplier: 1.2  # 20% higher tolerance during peak

  off_hours:
    schedule: "Mon-Fri 00:00-09:00,18:00-24:00 UTC"
    multiplier: 0.8  # 20% stricter during off-hours

  weekend:
    schedule: "Sat-Sun 00:00-24:00 UTC"
    multiplier: 1.5  # 50% higher tolerance on weekends

# Multi-condition alert rules to reduce false positives
alert_rules:
  # Critical Alerts - 0% false negative requirement
  - name: "critical_service_failure"
    severity: critical
    conditions:
      all_of:  # All conditions must be true
        - metric: error_rate
          operator: ">"
          threshold: 0.10  # 10% error rate
          for: 2m  # Sustained for 2 minutes
        - metric: success_count
          operator: "<"
          threshold: 10
          for: 1m
    aggregation:
      method: avg
      window: 60s
    suppression:
      enabled: true
      cooldown: 300s
    actions:
      - page_oncall
      - create_incident
    metadata:
      false_positive_weight: 0.01  # Very low tolerance
      business_impact: high

  - name: "critical_latency_spike"
    severity: critical
    conditions:
      all_of:
        - metric: p99_latency_ms
          operator: ">"
          threshold: 2000  # 2 seconds
          for: 3m
        - metric: p95_latency_ms
          operator: ">"
          threshold: 1500  # 1.5 seconds
          for: 3m
    dynamic_threshold:
      enabled: true
      baseline_window: 7d
      deviation_multiplier: 3  # 3x standard deviation
    suppression:
      enabled: true
      cooldown: 600s
    actions:
      - page_oncall
      - scale_replicas

  # High Priority Alerts
  - name: "high_error_rate"
    severity: high
    conditions:
      any_of:  # Any condition triggers
        - metric: error_rate
          operator: ">"
          threshold: 0.05  # 5% error rate
          for: 5m
        - metric: http_5xx_rate
          operator: ">"
          threshold: 0.03
          for: 3m
    aggregation:
      method: percentile
      percentile: 95
      window: 120s
    suppression:
      enabled: true
      cooldown: 900s
      group_by: [service, endpoint]
    actions:
      - notify_slack
      - create_jira_ticket

  - name: "memory_pressure"
    severity: high
    conditions:
      all_of:
        - metric: memory_usage_percent
          operator: ">"
          threshold: 85
          for: 5m
        - metric: memory_growth_rate
          operator: ">"
          threshold: 5  # 5% per minute
          for: 3m
    dynamic_threshold:
      enabled: true
      use_time_profile: true
    suppression:
      enabled: true
      cooldown: 1800s
    actions:
      - notify_slack
      - trigger_gc

  - name: "disk_space_critical"
    severity: high
    conditions:
      any_of:
        - metric: disk_usage_percent
          operator: ">"
          threshold: 90
          for: 2m
        - metric: disk_free_gb
          operator: "<"
          threshold: 5
          for: 1m
    predictive:
      enabled: true
      forecast_window: 1h
      alert_if_exhausted_within: 2h
    suppression:
      enabled: true
      cooldown: 3600s
    actions:
      - notify_slack
      - cleanup_old_logs

  # Medium Priority Alerts
  - name: "cache_degradation"
    severity: medium
    conditions:
      all_of:
        - metric: cache_hit_rate
          operator: "<"
          threshold: 0.60  # 60% hit rate
          for: 10m
        - metric: backend_latency_ms
          operator: ">"
          threshold: 500
          for: 5m
    aggregation:
      method: weighted_avg
      weight_by: request_count
      window: 300s
    suppression:
      enabled: true
      cooldown: 1800s
    actions:
      - notify_email
      - warm_cache

  - name: "slo_burn_rate_high"
    severity: medium
    conditions:
      any_of:
        - metric: slo_burn_rate_1h
          operator: ">"
          threshold: 14.4  # 14.4x burn rate
          for: 5m
        - metric: slo_burn_rate_6h
          operator: ">"
          threshold: 6
          for: 5m
    suppression:
      enabled: true
      cooldown: 3600s
      group_by: [slo_name]
    actions:
      - notify_slack
      - update_dashboard

  # Low Priority Alerts
  - name: "elevated_response_time"
    severity: low
    conditions:
      all_of:
        - metric: p50_latency_ms
          operator: ">"
          threshold: 200
          for: 15m
        - metric: request_rate
          operator: ">"
          threshold: 100  # Only alert if traffic is significant
          for: 15m
    dynamic_threshold:
      enabled: true
      baseline_window: 14d
      sensitivity: low
    suppression:
      enabled: true
      cooldown: 7200s
    actions:
      - log_metric
      - notify_dashboard

  - name: "cpu_usage_elevated"
    severity: low
    conditions:
      all_of:
        - metric: cpu_usage_percent
          operator: ">"
          threshold: 70
          for: 20m
        - metric: cpu_throttle_percent
          operator: "<"
          threshold: 5  # Not throttling yet
          for: 20m
    suppression:
      enabled: true
      cooldown: 3600s
    actions:
      - log_metric
      - suggest_scaling

# Suppression rules for known issues and maintenance
suppression_rules:
  - name: "scheduled_maintenance"
    enabled: true
    schedule:
      recurring: "Sun 02:00-04:00 UTC"
    suppress:
      - all_alerts
    reason: "Weekly maintenance window"

  - name: "known_third_party_flakiness"
    enabled: true
    conditions:
      - metric: external_api_error_rate
        operator: ">"
        threshold: 0.20
    suppress:
      - "high_error_rate"
      - "critical_service_failure"
    max_duration: 3600s
    reason: "Known third-party API issues"

  - name: "cold_start_suppression"
    enabled: true
    trigger: "deployment_completed"
    duration: 300s  # 5 minutes after deployment
    suppress:
      - "elevated_response_time"
      - "cache_degradation"
    reason: "Expected cold start behavior"

  - name: "batch_job_suppression"
    enabled: true
    schedule:
      cron: "0 2 * * *"  # 2 AM daily
    duration: 1800s  # 30 minutes
    suppress:
      - "cpu_usage_elevated"
      - "memory_pressure"
    reason: "Batch processing window"

# Aggregation rules to reduce alert noise
aggregation_rules:
  - name: "group_by_service"
    group_by: [service_name, environment]
    window: 300s
    min_count: 3  # Need at least 3 instances before alerting

  - name: "group_by_endpoint"
    group_by: [endpoint, method]
    window: 180s
    min_count: 5

  - name: "group_by_host"
    group_by: [hostname, datacenter]
    window: 600s
    min_count: 2

# Correlation rules to identify root causes
correlation_rules:
  - name: "cascade_detection"
    parent_alert: "database_connection_failure"
    child_alerts:
      - "high_error_rate"
      - "critical_service_failure"
    suppression_window: 120s

  - name: "resource_correlation"
    correlate:
      - "memory_pressure"
      - "cpu_usage_elevated"
      - "elevated_response_time"
    window: 300s
    create_composite_alert: true

# Adaptive thresholds based on historical data
adaptive_thresholds:
  enabled: true
  algorithms:
    - name: "seasonal_decomposition"
      metrics:
        - request_rate
        - error_rate
        - latency_p99
      seasonality: [daily, weekly]

    - name: "anomaly_detection"
      method: "isolation_forest"
      metrics:
        - all_metrics
      sensitivity: 0.95  # 95% confidence

    - name: "trend_analysis"
      method: "linear_regression"
      metrics:
        - disk_usage_percent
        - memory_usage_percent
      window: 7d
      forecast: 24h

# Alert routing and escalation
routing:
  - match:
      severity: critical
    routes:
      - pagerduty
      - slack_critical
    escalation:
      delay: 300s
      to: engineering_manager

  - match:
      severity: high
    routes:
      - slack_alerts
      - email_oncall
    escalation:
      delay: 1800s
      to: team_lead

  - match:
      severity: medium
    routes:
      - slack_alerts
      - jira
    escalation:
      delay: 3600s
      to: team

  - match:
      severity: low
    routes:
      - logs
      - metrics_dashboard

# Performance optimization settings
performance:
  # Batch processing for efficiency
  batch_size: 100
  batch_timeout: 5s

  # Caching for frequently accessed metrics
  cache:
    enabled: true
    ttl: 60s
    size: 10000

  # Rate limiting to prevent alert storms
  rate_limiting:
    enabled: true
    max_alerts_per_minute: 50
    max_alerts_per_rule: 10

  # Circuit breaker for external notifications
  circuit_breaker:
    enabled: true
    failure_threshold: 5
    timeout: 30s
    half_open_requests: 3

# Metrics for monitoring the alerting system itself
meta_monitoring:
  enabled: true
  metrics:
    - alert_evaluation_duration_ms
    - alert_notification_success_rate
    - alert_suppression_rate
    - false_positive_rate
    - alert_noise_ratio
  thresholds:
    alert_evaluation_duration_ms: 100
    alert_notification_success_rate: 0.99
    false_positive_rate: 0.05  # Target <5%
    cpu_overhead_percent: 2.0  # Target <2%