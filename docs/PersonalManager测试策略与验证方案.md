# PersonalManageræµ‹è¯•ç­–ç•¥ä¸éªŒè¯æ–¹æ¡ˆ

> **ç‰ˆæœ¬**: v1.0  
> **åˆ›å»ºæ—¥æœŸ**: 2025-09-11  
> **æµ‹è¯•æ¡†æ¶**: Jest + Python pytest + Bash bats  
> **è¦†ç›–ç‡ç›®æ ‡**: >90%  

## ğŸ“‹ ç›®å½•

1. [æµ‹è¯•ç­–ç•¥æ¦‚è§ˆ](#æµ‹è¯•ç­–ç•¥æ¦‚è§ˆ)
2. [å•å…ƒæµ‹è¯•æ–¹æ¡ˆ](#å•å…ƒæµ‹è¯•æ–¹æ¡ˆ)
3. [åŠŸèƒ½æµ‹è¯•æ–¹æ¡ˆ](#åŠŸèƒ½æµ‹è¯•æ–¹æ¡ˆ)
4. [é›†æˆæµ‹è¯•æ–¹æ¡ˆ](#é›†æˆæµ‹è¯•æ–¹æ¡ˆ)
5. [APIé›†æˆæµ‹è¯•](#apié›†æˆæµ‹è¯•)
6. [ç”¨æˆ·éªŒæ”¶æµ‹è¯•](#ç”¨æˆ·éªŒæ”¶æµ‹è¯•)
7. [æ€§èƒ½æµ‹è¯•æ–¹æ¡ˆ](#æ€§èƒ½æµ‹è¯•æ–¹æ¡ˆ)
8. [CLIç‰¹æ®Šæµ‹è¯•](#cliç‰¹æ®Šæµ‹è¯•)
9. [è‡ªåŠ¨åŒ–æµ‹è¯•æ¶æ„](#è‡ªåŠ¨åŒ–æµ‹è¯•æ¶æ„)
10. [è´¨é‡ä¿éšœæµç¨‹](#è´¨é‡ä¿éšœæµç¨‹)

---

## ğŸ¯ æµ‹è¯•ç­–ç•¥æ¦‚è§ˆ

### æµ‹è¯•é‡‘å­—å¡”æ¶æ„

```mermaid
graph TB
    A[æ‰‹åŠ¨æ¢ç´¢æµ‹è¯•] --> B[ç”¨æˆ·éªŒæ”¶æµ‹è¯•]
    B --> C[ç«¯åˆ°ç«¯æµ‹è¯•]
    C --> D[é›†æˆæµ‹è¯•]
    D --> E[å•å…ƒæµ‹è¯•]
    
    E --> E1[Agenté€»è¾‘æµ‹è¯•<br/>70% è¦†ç›–ç‡]
    D --> D1[Agenté—´é€šä¿¡<br/>85% è¦†ç›–ç‡]
    C --> C1[å®Œæ•´å·¥ä½œæµç¨‹<br/>95% è¦†ç›–ç‡]
    B --> B1[çœŸå®ä½¿ç”¨åœºæ™¯<br/>100% è¦†ç›–ç‡]
    A --> A1[è¾¹ç•Œæƒ…å†µå‘ç°<br/>æŒç»­è¿›è¡Œ]
```

### æµ‹è¯•åˆ†å±‚ç­–ç•¥

| æµ‹è¯•å±‚çº§ | ç›®æ ‡ | è¦†ç›–ç‡ | æ‰§è¡Œé¢‘ç‡ | å·¥å…· |
|----------|------|--------|----------|------|
| **å•å…ƒæµ‹è¯•** | éªŒè¯å•ä¸ªå‡½æ•°/æ–¹æ³• | 70% | æ¯æ¬¡æäº¤ | Jest, pytest |
| **åŠŸèƒ½æµ‹è¯•** | éªŒè¯å•ä¸ªåŠŸèƒ½æ¨¡å— | 85% | æ¯æ¬¡æäº¤ | Jest, bats |
| **é›†æˆæµ‹è¯•** | éªŒè¯Agenté—´åä½œ | 90% | æ¯æ—¥æ„å»º | Python + Mock |
| **APIæµ‹è¯•** | éªŒè¯å¤–éƒ¨APIé›†æˆ | 95% | æ¯æ—¥æ„å»º | Python requests |
| **ç”¨æˆ·æµ‹è¯•** | éªŒè¯ç”¨æˆ·ä½“éªŒ | 100% | æ¯å‘¨æ‰§è¡Œ | æ‰‹åŠ¨+è‡ªåŠ¨åŒ– |
| **æ€§èƒ½æµ‹è¯•** | éªŒè¯æ€§èƒ½æŒ‡æ ‡ | å…³é”®è·¯å¾„ | æ¯å‘¨æ‰§è¡Œ | Artillery, ab |

### æµ‹è¯•æ•°æ®ç®¡ç†ç­–ç•¥

```yaml
test_data_strategy:
  # æµ‹è¯•æ•°æ®åˆ†ç±»
  data_categories:
    minimal_dataset:         # æœ€å°æ•°æ®é›†
      projects: 3           # 3ä¸ªé¡¹ç›®
      tasks: 15            # 15ä¸ªä»»åŠ¡
      goals: 5             # 5ä¸ªç›®æ ‡
      habits: 3            # 3ä¸ªä¹ æƒ¯
      duration: "1å‘¨å†å²æ•°æ®"
      
    realistic_dataset:      # çœŸå®æ•°æ®é›†
      projects: 10          # 10ä¸ªé¡¹ç›®
      tasks: 50            # 50ä¸ªä»»åŠ¡
      goals: 15            # 15ä¸ªç›®æ ‡
      habits: 8            # 8ä¸ªä¹ æƒ¯
      duration: "3ä¸ªæœˆå†å²æ•°æ®"
      
    stress_dataset:        # å‹åŠ›æµ‹è¯•æ•°æ®é›†
      projects: 100         # 100ä¸ªé¡¹ç›®
      tasks: 1000          # 1000ä¸ªä»»åŠ¡
      goals: 200           # 200ä¸ªç›®æ ‡
      habits: 50           # 50ä¸ªä¹ æƒ¯
      duration: "1å¹´å†å²æ•°æ®"
  
  # æ•°æ®ç”Ÿæˆç­–ç•¥
  data_generation:
    synthetic_data: true    # åˆæˆæ•°æ®
    anonymized_real_data: false  # åŒ¿åçœŸå®æ•°æ®
    template_based: true    # åŸºäºæ¨¡æ¿
    realistic_patterns: true # çœŸå®ä½¿ç”¨æ¨¡å¼
```

---

## ğŸ”¬ å•å…ƒæµ‹è¯•æ–¹æ¡ˆ

### 1. Agentå•å…ƒæµ‹è¯•

```javascript
// PersonalManagerAgent å•å…ƒæµ‹è¯•ç¤ºä¾‹
describe('PersonalManagerAgent', () => {
  let agent;
  
  beforeEach(() => {
    agent = new PersonalManagerAgent();
    agent.initialize(mockConfig);
  });

  describe('ä»»åŠ¡è§£æåŠŸèƒ½', () => {
    test('åº”è¯¥æ­£ç¡®è§£æç”¨æˆ·æ„å›¾', async () => {
      const userInput = "ä»Šå¤©åº”è¯¥åšä»€ä¹ˆ";
      const result = await agent.parseUserIntent(userInput);
      
      expect(result.intent).toBe('get_task_recommendations');
      expect(result.confidence).toBeGreaterThan(0.8);
      expect(result.parameters).toEqual({
        timeframe: 'today',
        context: 'work_planning'
      });
    });
    
    test('åº”è¯¥å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢', async () => {
      const userInput = "å¸®æˆ‘çœ‹çœ‹é¡¹ç›®æ€ä¹ˆæ ·äº†";
      const result = await agent.parseUserIntent(userInput);
      
      expect(result.intent).toBe('get_project_status');
      expect(result.confidence).toBeGreaterThan(0.7);
    });
  });

  describe('Agentåè°ƒåŠŸèƒ½', () => {
    test('åº”è¯¥æ­£ç¡®è·¯ç”±åˆ°TaskAgent', async () => {
      const mockTaskAgent = jest.fn();
      agent.registerAgent('TaskAgent', mockTaskAgent);
      
      await agent.processRequest({
        intent: 'get_task_recommendations',
        parameters: { timeframe: 'today' }
      });
      
      expect(mockTaskAgent).toHaveBeenCalledWith({
        action: 'get_recommendations',
        timeframe: 'today'
      });
    });
  });
});
```

```python
# PriorityAgent å•å…ƒæµ‹è¯•ç¤ºä¾‹
import pytest
from agents.priority_agent import PriorityAgent
from models.task import Task
from models.priority import PriorityCalculation

class TestPriorityAgent:
    def setup_method(self):
        self.agent = PriorityAgent()
        self.sample_tasks = [
            Task(
                id="task-1",
                title="ç´§æ€¥bugä¿®å¤",
                deadline="2025-09-12T18:00:00Z",
                importance=9,
                urgency=10,
                estimated_duration=120
            ),
            Task(
                id="task-2", 
                title="åŠŸèƒ½ä¼˜åŒ–",
                deadline="2025-09-20T18:00:00Z",
                importance=6,
                urgency=4,
                estimated_duration=240
            )
        ]
    
    def test_calculate_urgency_score(self):
        """æµ‹è¯•ç´§æ€¥ç¨‹åº¦è®¡ç®—"""
        task = self.sample_tasks[0]
        score = self.agent.calculate_urgency_score(
            task, 
            current_time="2025-09-11T18:00:00Z"
        )
        
        # 24å°æ—¶å†…æˆªæ­¢ï¼Œåº”è¯¥æ˜¯é«˜ç´§æ€¥åº¦
        assert score >= 90
        assert score <= 100
    
    def test_priority_ranking(self):
        """æµ‹è¯•ä¼˜å…ˆçº§æ’åº"""
        result = self.agent.calculate_priorities(
            self.sample_tasks,
            context={
                "current_time": "2025-09-11T18:00:00Z",
                "available_time": 180,
                "energy_level": 8
            }
        )
        
        # ç´§æ€¥ä»»åŠ¡åº”è¯¥æ’åœ¨å‰é¢
        assert result.ranked_tasks[0].task_id == "task-1"
        assert result.ranked_tasks[0].final_priority_score > 80
    
    def test_algorithm_weights(self):
        """æµ‹è¯•ç®—æ³•æƒé‡é…ç½®"""
        # æµ‹è¯•é«˜ç´§æ€¥æƒé‡é…ç½®
        weights_urgent = {
            "urgency_weight": 0.5,
            "importance_weight": 0.2,
            "effort_weight": 0.3
        }
        
        result1 = self.agent.calculate_priorities(
            self.sample_tasks, 
            weights=weights_urgent
        )
        
        # æµ‹è¯•é«˜é‡è¦æ€§æƒé‡é…ç½®
        weights_important = {
            "urgency_weight": 0.2,
            "importance_weight": 0.5, 
            "effort_weight": 0.3
        }
        
        result2 = self.agent.calculate_priorities(
            self.sample_tasks,
            weights=weights_important
        )
        
        # ä¸åŒæƒé‡åº”è¯¥äº§ç”Ÿä¸åŒçš„æ’åºç»“æœ
        assert result1.ranked_tasks[0].final_priority_score != \
               result2.ranked_tasks[0].final_priority_score
```

### 2. æ•°æ®æ¨¡å‹æµ‹è¯•

```javascript
// æ•°æ®éªŒè¯æµ‹è¯•
describe('æ•°æ®æ¨¡å‹éªŒè¯', () => {
  describe('Projectæ¨¡å‹', () => {
    test('åº”è¯¥æ‹’ç»æ— æ•ˆçš„UUID', () => {
      expect(() => {
        new Project({
          id: 'invalid-uuid',
          name: 'Test Project'
        });
      }).toThrow('Invalid UUID format');
    });
    
    test('åº”è¯¥éªŒè¯æ—¥æœŸèŒƒå›´', () => {
      expect(() => {
        new Project({
          id: generateUUID(),
          name: 'Test Project',
          dates: {
            started_at: '2025-09-30',
            deadline: '2025-09-01'  // æˆªæ­¢æ—¥æœŸæ—©äºå¼€å§‹æ—¥æœŸ
          }
        });
      }).toThrow('Deadline cannot be before start date');
    });
    
    test('åº”è¯¥è®¡ç®—æ­£ç¡®çš„å®Œæˆç™¾åˆ†æ¯”', () => {
      const project = new Project({
        id: generateUUID(),
        name: 'Test Project',
        progress: {
          milestones_completed: 3,
          milestones_total: 5
        }
      });
      
      expect(project.calculateCompletionPercentage()).toBe(60);
    });
  });
  
  describe('ä¼˜å…ˆçº§è®¡ç®—éªŒè¯', () => {
    test('ä¼˜å…ˆçº§åˆ†æ•°åº”è¯¥åœ¨æœ‰æ•ˆèŒƒå›´å†…', () => {
      const calculation = new PriorityCalculation({
        urgency: 10,
        importance: 8,
        effort: 5,
        alignment: 9
      });
      
      const score = calculation.calculateFinalScore();
      expect(score).toBeGreaterThanOrEqual(0);
      expect(score).toBeLessThanOrEqual(100);
    });
  });
});
```

### 3. å·¥å…·å‡½æ•°æµ‹è¯•

```bash
#!/usr/bin/env bats
# CLIå·¥å…·å‡½æ•°æµ‹è¯•

@test "æ—¶é—´è§£æå‡½æ•°åº”è¯¥æ­£ç¡®è§£æå„ç§æ ¼å¼" {
  source src/utils/time_parser.sh
  
  result=$(parse_relative_time "ä»Šå¤©")
  [ "$result" = "$(date +%Y-%m-%d)" ]
  
  result=$(parse_relative_time "æ˜å¤©") 
  expected=$(date -d "tomorrow" +%Y-%m-%d)
  [ "$result" = "$expected" ]
  
  result=$(parse_relative_time "ä¸‹å‘¨")
  [ -n "$result" ]  # åº”è¯¥è¿”å›éç©ºç»“æœ
}

@test "é¡¹ç›®è·¯å¾„æ£€æµ‹åº”è¯¥è¯†åˆ«Gitä»“åº“" {
  source src/utils/project_detector.sh
  
  # åˆ›å»ºä¸´æ—¶Gitä»“åº“
  tmpdir=$(mktemp -d)
  cd "$tmpdir"
  git init
  
  result=$(detect_project_type)
  [ "$result" = "git_repository" ]
  
  # æ¸…ç†
  rm -rf "$tmpdir"
}

@test "é…ç½®æ–‡ä»¶éªŒè¯åº”è¯¥æ£€æµ‹æ ¼å¼é”™è¯¯" {
  source src/utils/config_validator.sh
  
  # æµ‹è¯•æœ‰æ•ˆé…ç½®
  echo "user_id: test-123" > valid_config.yaml
  run validate_config_file valid_config.yaml
  [ "$status" -eq 0 ]
  
  # æµ‹è¯•æ— æ•ˆé…ç½®
  echo "invalid yaml: [" > invalid_config.yaml  
  run validate_config_file invalid_config.yaml
  [ "$status" -ne 0 ]
  
  # æ¸…ç†
  rm -f valid_config.yaml invalid_config.yaml
}
```

---

## âš™ï¸ åŠŸèƒ½æµ‹è¯•æ–¹æ¡ˆ

### 1. æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•å¥—ä»¶

```python
# åŠŸèƒ½æµ‹è¯• - ä»»åŠ¡ç®¡ç†
import pytest
from personal_manager import PersonalManager
from test_utils import create_test_environment, cleanup_test_environment

class TestTaskManagement:
    def setup_method(self):
        self.pm = PersonalManager()
        self.test_env = create_test_environment()
        
    def teardown_method(self):
        cleanup_test_environment(self.test_env)
    
    def test_create_task_workflow(self):
        """æµ‹è¯•åˆ›å»ºä»»åŠ¡çš„å®Œæ•´å·¥ä½œæµ"""
        # 1. åˆ›å»ºé¡¹ç›®
        project = self.pm.create_project({
            "name": "æµ‹è¯•é¡¹ç›®",
            "description": "åŠŸèƒ½æµ‹è¯•ç”¨é¡¹ç›®"
        })
        assert project.id is not None
        
        # 2. åˆ›å»ºä»»åŠ¡
        task = self.pm.create_task({
            "title": "æµ‹è¯•ä»»åŠ¡",
            "project_id": project.id,
            "priority": "high",
            "estimated_duration": 120
        })
        assert task.id is not None
        assert task.project_id == project.id
        
        # 3. éªŒè¯ä»»åŠ¡å‡ºç°åœ¨é¡¹ç›®ä¸­
        project_tasks = self.pm.get_project_tasks(project.id)
        assert len(project_tasks) == 1
        assert project_tasks[0].id == task.id
    
    def test_task_priority_calculation(self):
        """æµ‹è¯•ä»»åŠ¡ä¼˜å…ˆçº§è‡ªåŠ¨è®¡ç®—"""
        # åˆ›å»ºç´§æ€¥ä»»åŠ¡
        urgent_task = self.pm.create_task({
            "title": "ç´§æ€¥ä»»åŠ¡",
            "deadline": "2025-09-12T18:00:00Z",  # æ˜å¤©æˆªæ­¢
            "importance": 9,
            "urgency": 10
        })
        
        # åˆ›å»ºæ™®é€šä»»åŠ¡
        normal_task = self.pm.create_task({
            "title": "æ™®é€šä»»åŠ¡", 
            "deadline": "2025-09-20T18:00:00Z",  # ä¸‹å‘¨æˆªæ­¢
            "importance": 6,
            "urgency": 5
        })
        
        # è·å–æ¨èä»»åŠ¡
        recommendations = self.pm.get_task_recommendations({
            "timeframe": "today",
            "max_tasks": 5
        })
        
        # ç´§æ€¥ä»»åŠ¡åº”è¯¥æ’åœ¨å‰é¢
        assert recommendations[0].id == urgent_task.id
        assert recommendations[0].priority_score > recommendations[1].priority_score
    
    def test_task_status_transitions(self):
        """æµ‹è¯•ä»»åŠ¡çŠ¶æ€è½¬æ¢"""
        task = self.pm.create_task({
            "title": "çŠ¶æ€æµ‹è¯•ä»»åŠ¡",
            "status": "todo"
        })
        
        # å¼€å§‹ä»»åŠ¡
        self.pm.start_task(task.id)
        updated_task = self.pm.get_task(task.id)
        assert updated_task.status == "in_progress"
        assert updated_task.actual_start is not None
        
        # å®Œæˆä»»åŠ¡
        self.pm.complete_task(task.id)
        completed_task = self.pm.get_task(task.id)
        assert completed_task.status == "completed"
        assert completed_task.completed_at is not None
        assert completed_task.actual_duration > 0

class TestProjectManagement:
    def test_project_status_auto_update(self):
        """æµ‹è¯•é¡¹ç›®çŠ¶æ€è‡ªåŠ¨æ›´æ–°"""
        pm = PersonalManager()
        
        # åˆ›å»ºé¡¹ç›®å’Œä»»åŠ¡
        project = pm.create_project({"name": "è‡ªåŠ¨æ›´æ–°æµ‹è¯•é¡¹ç›®"})
        
        tasks = []
        for i in range(5):
            task = pm.create_task({
                "title": f"ä»»åŠ¡ {i+1}",
                "project_id": project.id
            })
            tasks.append(task)
        
        # å®Œæˆéƒ¨åˆ†ä»»åŠ¡
        pm.complete_task(tasks[0].id)
        pm.complete_task(tasks[1].id)
        
        # æ£€æŸ¥é¡¹ç›®è¿›åº¦æ›´æ–°
        updated_project = pm.get_project(project.id)
        assert updated_project.progress.completion_percentage == 40  # 2/5 = 40%
        
        # å®Œæˆæ‰€æœ‰ä»»åŠ¡
        for task in tasks[2:]:
            pm.complete_task(task.id)
        
        # æ£€æŸ¥é¡¹ç›®çŠ¶æ€
        final_project = pm.get_project(project.id)
        assert final_project.status == "completed"
        assert final_project.progress.completion_percentage == 100
```

### 2. è¾¹ç•Œæƒ…å†µæµ‹è¯•

```python
class TestBoundaryConditions:
    def test_empty_input_handling(self):
        """æµ‹è¯•ç©ºè¾“å…¥å¤„ç†"""
        pm = PersonalManager()
        
        # ç©ºå­—ç¬¦ä¸²è¾“å…¥
        result = pm.process_command("")
        assert result.success == False
        assert "empty" in result.error_message.lower()
        
        # ç©ºç™½å­—ç¬¦ä¸²è¾“å…¥
        result = pm.process_command("   \n\t   ")
        assert result.success == False
        
        # Noneè¾“å…¥
        result = pm.process_command(None)
        assert result.success == False
    
    def test_large_dataset_handling(self):
        """æµ‹è¯•å¤§æ•°æ®é›†å¤„ç†"""
        pm = PersonalManager()
        
        # åˆ›å»ºå¤§é‡ä»»åŠ¡
        project = pm.create_project({"name": "å‹åŠ›æµ‹è¯•é¡¹ç›®"})
        
        task_ids = []
        for i in range(1000):
            task = pm.create_task({
                "title": f"æ‰¹é‡ä»»åŠ¡ {i+1}",
                "project_id": project.id
            })
            task_ids.append(task.id)
        
        # æµ‹è¯•æ‰¹é‡æ“ä½œæ€§èƒ½
        import time
        start_time = time.time()
        
        recommendations = pm.get_task_recommendations({
            "max_tasks": 10
        })
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        # åº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆï¼ˆ<2ç§’ï¼‰
        assert processing_time < 2.0
        assert len(recommendations) == 10
    
    def test_invalid_data_rejection(self):
        """æµ‹è¯•æ— æ•ˆæ•°æ®æ‹’ç»"""
        pm = PersonalManager()
        
        # æ— æ•ˆUUID
        with pytest.raises(ValueError, match="Invalid UUID"):
            pm.get_task("invalid-uuid")
        
        # æ— æ•ˆæ—¥æœŸæ ¼å¼
        with pytest.raises(ValueError, match="Invalid date"):
            pm.create_task({
                "title": "æµ‹è¯•ä»»åŠ¡",
                "deadline": "not-a-date"
            })
        
        # è¶…å‡ºèŒƒå›´çš„ä¼˜å…ˆçº§
        with pytest.raises(ValueError, match="Priority.*range"):
            pm.create_task({
                "title": "æµ‹è¯•ä»»åŠ¡",
                "priority_score": 150  # è¶…å‡º0-100èŒƒå›´
            })
```

---

## ğŸ”— é›†æˆæµ‹è¯•æ–¹æ¡ˆ

### 1. Agenté—´é€šä¿¡æµ‹è¯•

```python
# Agenté›†æˆæµ‹è¯•
import pytest
from unittest.mock import MagicMock, patch
from agents.personal_manager_agent import PersonalManagerAgent
from agents.task_agent import TaskAgent
from agents.priority_agent import PriorityAgent
from agents.project_agent import ProjectAgent

class TestAgentIntegration:
    def setup_method(self):
        self.pm_agent = PersonalManagerAgent()
        self.task_agent = TaskAgent()
        self.priority_agent = PriorityAgent()
        self.project_agent = ProjectAgent()
        
        # æ³¨å†Œæ‰€æœ‰Agent
        self.pm_agent.register_agent("TaskAgent", self.task_agent)
        self.pm_agent.register_agent("PriorityAgent", self.priority_agent)
        self.pm_agent.register_agent("ProjectAgent", self.project_agent)
    
    def test_task_recommendation_workflow(self):
        """æµ‹è¯•ä»»åŠ¡æ¨èçš„å®Œæ•´å·¥ä½œæµ"""
        # æ¨¡æ‹Ÿç”¨æˆ·è¯·æ±‚
        user_request = {
            "command": "ä»Šå¤©åº”è¯¥åšä»€ä¹ˆ",
            "user_id": "test-user-123"
        }
        
        # æ‰§è¡Œè¯·æ±‚
        response = self.pm_agent.process_request(user_request)
        
        # éªŒè¯å“åº”
        assert response.success == True
        assert "recommendations" in response.data
        assert len(response.data["recommendations"]) > 0
        
        # éªŒè¯Agenté—´é€šä¿¡
        assert self.task_agent.last_request is not None
        assert self.priority_agent.last_request is not None
    
    def test_project_status_update_chain(self):
        """æµ‹è¯•é¡¹ç›®çŠ¶æ€æ›´æ–°é“¾è·¯"""
        # æ¨¡æ‹ŸGitæäº¤è§¦å‘é¡¹ç›®æ›´æ–°
        git_event = {
            "event_type": "git_commit",
            "project_path": "./test_project",
            "commit_hash": "abc123",
            "commit_message": "å®Œæˆç”¨æˆ·è®¤è¯åŠŸèƒ½",
            "files_changed": 3
        }
        
        # è§¦å‘é¡¹ç›®çŠ¶æ€æ›´æ–°
        response = self.project_agent.handle_git_event(git_event)
        
        # éªŒè¯çŠ¶æ€æ›´æ–°
        assert response.success == True
        assert "status_updated" in response.data
        
        # éªŒè¯åç»­è§¦å‘çš„ä¼˜å…ˆçº§é‡è®¡ç®—
        priority_requests = self.priority_agent.get_recent_requests()
        assert len(priority_requests) > 0
        assert priority_requests[0]["trigger"] == "project_status_change"
    
    def test_error_propagation(self):
        """æµ‹è¯•é”™è¯¯ä¼ æ’­æœºåˆ¶"""
        # æ¨¡æ‹ŸTaskAgentå‡ºé”™
        with patch.object(self.task_agent, 'get_tasks') as mock_get_tasks:
            mock_get_tasks.side_effect = Exception("Database connection failed")
            
            # å¤„ç†è¯·æ±‚
            response = self.pm_agent.process_request({
                "command": "æ˜¾ç¤ºæˆ‘çš„ä»»åŠ¡",
                "user_id": "test-user"
            })
            
            # éªŒè¯é”™è¯¯å¤„ç†
            assert response.success == False
            assert "Database connection failed" in response.error_message
            assert response.error_code == "AGENT_COMMUNICATION_ERROR"
```

### 2. æ•°æ®æµé›†æˆæµ‹è¯•

```python
class TestDataFlowIntegration:
    def test_end_to_end_task_lifecycle(self):
        """æµ‹è¯•ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸçš„ç«¯åˆ°ç«¯æ•°æ®æµ"""
        pm = PersonalManager()
        
        # 1. åˆ›å»ºé¡¹ç›® -> æ•°æ®åº”è¯¥åœ¨ProjectAgentä¸­
        project_data = {
            "name": "é›†æˆæµ‹è¯•é¡¹ç›®",
            "description": "ç«¯åˆ°ç«¯æµ‹è¯•ç”¨é¡¹ç›®"
        }
        project = pm.create_project(project_data)
        
        # éªŒè¯ProjectAgentä¸­çš„æ•°æ®
        project_agent = pm.get_agent("ProjectAgent")
        stored_project = project_agent.get_project(project.id)
        assert stored_project.name == project_data["name"]
        
        # 2. åˆ›å»ºä»»åŠ¡ -> æ•°æ®åº”è¯¥åœ¨TaskAgentä¸­
        task_data = {
            "title": "é›†æˆæµ‹è¯•ä»»åŠ¡",
            "project_id": project.id,
            "deadline": "2025-09-12T18:00:00Z"
        }
        task = pm.create_task(task_data)
        
        # éªŒè¯TaskAgentä¸­çš„æ•°æ®
        task_agent = pm.get_agent("TaskAgent")
        stored_task = task_agent.get_task(task.id)
        assert stored_task.project_id == project.id
        
        # 3. è®¡ç®—ä¼˜å…ˆçº§ -> æ•°æ®åº”è¯¥åœ¨PriorityAgentä¸­  
        priority_result = pm.calculate_task_priorities([task.id])
        
        # éªŒè¯PriorityAgentä¸­çš„æ•°æ®
        priority_agent = pm.get_agent("PriorityAgent")
        calculation = priority_agent.get_latest_calculation()
        assert task.id in [t.task_id for t in calculation.ranked_tasks]
        
        # 4. æ›´æ–°ä»»åŠ¡çŠ¶æ€ -> æ•°æ®åº”è¯¥è·¨AgentåŒæ­¥
        pm.start_task(task.id)
        
        # éªŒè¯æ•°æ®åŒæ­¥
        updated_task = task_agent.get_task(task.id)
        project_tasks = project_agent.get_project_tasks(project.id)
        
        assert updated_task.status == "in_progress"
        assert project_tasks[0].status == "in_progress"
    
    def test_data_consistency_across_agents(self):
        """æµ‹è¯•è·¨Agentçš„æ•°æ®ä¸€è‡´æ€§"""
        pm = PersonalManager()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        project = pm.create_project({"name": "ä¸€è‡´æ€§æµ‹è¯•é¡¹ç›®"})
        task1 = pm.create_task({"title": "ä»»åŠ¡1", "project_id": project.id})
        task2 = pm.create_task({"title": "ä»»åŠ¡2", "project_id": project.id})
        
        # ä»ä¸åŒAgentè·å–ç›¸åŒæ•°æ®
        task_agent = pm.get_agent("TaskAgent")
        project_agent = pm.get_agent("ProjectAgent")
        
        task1_from_task_agent = task_agent.get_task(task1.id)
        project_tasks = project_agent.get_project_tasks(project.id)
        task1_from_project_agent = next(t for t in project_tasks if t.id == task1.id)
        
        # éªŒè¯æ•°æ®ä¸€è‡´æ€§
        assert task1_from_task_agent.title == task1_from_project_agent.title
        assert task1_from_task_agent.status == task1_from_project_agent.status
        assert task1_from_task_agent.updated_at == task1_from_project_agent.updated_at
```

---

## ğŸŒ APIé›†æˆæµ‹è¯•

### 1. Google APIsé›†æˆæµ‹è¯•

```python
# Google APIsé›†æˆæµ‹è¯•
import pytest
from unittest.mock import patch, MagicMock
from integrations.google_calendar import GoogleCalendarIntegration
from integrations.google_tasks import GoogleTasksIntegration
from integrations.google_gmail import GoogleGmailIntegration

class TestGoogleAPIIntegration:
    def setup_method(self):
        self.calendar_integration = GoogleCalendarIntegration(
            credentials_file="test_credentials.json"
        )
        self.tasks_integration = GoogleTasksIntegration(
            credentials_file="test_credentials.json"
        )
    
    @patch('google.auth.default')
    @patch('googleapiclient.discovery.build')
    def test_calendar_event_sync(self, mock_build, mock_auth):
        """æµ‹è¯•æ—¥å†äº‹ä»¶åŒæ­¥"""
        # Mock Google Calendar APIå“åº”
        mock_service = MagicMock()
        mock_build.return_value = mock_service
        
        mock_events = {
            'items': [{
                'id': 'event123',
                'summary': 'é¡¹ç›®ä¼šè®®',
                'start': {'dateTime': '2025-09-12T14:00:00Z'},
                'end': {'dateTime': '2025-09-12T15:00:00Z'},
                'attendees': [{'email': 'test@example.com'}]
            }]
        }
        
        mock_service.events().list().execute.return_value = mock_events
        
        # æ‰§è¡ŒåŒæ­¥
        sync_result = self.calendar_integration.sync_events(
            start_date='2025-09-12',
            end_date='2025-09-13'
        )
        
        # éªŒè¯åŒæ­¥ç»“æœ
        assert sync_result.success == True
        assert len(sync_result.synced_events) == 1
        assert sync_result.synced_events[0]['summary'] == 'é¡¹ç›®ä¼šè®®'
    
    def test_api_error_handling(self):
        """æµ‹è¯•APIé”™è¯¯å¤„ç†"""
        with patch('googleapiclient.discovery.build') as mock_build:
            # æ¨¡æ‹ŸAPIé”™è¯¯
            mock_service = MagicMock()
            mock_build.return_value = mock_service
            mock_service.events().list().execute.side_effect = Exception("API Rate limit exceeded")
            
            # æ‰§è¡ŒåŒæ­¥ï¼Œåº”è¯¥ä¼˜é›…å¤„ç†é”™è¯¯
            sync_result = self.calendar_integration.sync_events(
                start_date='2025-09-12',
                end_date='2025-09-13'
            )
            
            # éªŒè¯é”™è¯¯å¤„ç†
            assert sync_result.success == False
            assert "rate limit" in sync_result.error_message.lower()
            assert sync_result.error_code == "API_RATE_LIMIT"
    
    def test_data_transformation(self):
        """æµ‹è¯•APIæ•°æ®è½¬æ¢"""
        # Google Calendaräº‹ä»¶æ ¼å¼
        google_event = {
            'id': 'cal_event_123',
            'summary': 'æŠ€æœ¯è¯„å®¡ä¼šè®®',
            'start': {'dateTime': '2025-09-12T10:00:00+08:00'},
            'end': {'dateTime': '2025-09-12T11:30:00+08:00'},
            'description': 'è®¨è®ºæ–°åŠŸèƒ½æŠ€æœ¯æ–¹æ¡ˆ',
            'location': 'ä¼šè®®å®¤A'
        }
        
        # è½¬æ¢ä¸ºPersonalManagerä»»åŠ¡æ ¼å¼
        pm_task = self.calendar_integration.transform_event_to_task(google_event)
        
        # éªŒè¯è½¬æ¢ç»“æœ
        assert pm_task.title == 'æŠ€æœ¯è¯„å®¡ä¼šè®®'
        assert pm_task.context == 'meeting'
        assert pm_task.scheduled_start == '2025-09-12T02:00:00.000Z'  # UTCæ—¶é—´
        assert pm_task.scheduled_end == '2025-09-12T03:30:00.000Z'
        assert pm_task.location == 'ä¼šè®®å®¤A'
        assert pm_task.google_event_id == 'cal_event_123'

class TestGmailIntegration:
    def test_important_email_detection(self):
        """æµ‹è¯•é‡è¦é‚®ä»¶æ£€æµ‹"""
        gmail = GoogleGmailIntegration()
        
        # é‡è¦é‚®ä»¶ç¤ºä¾‹
        important_email = {
            'id': 'email123',
            'subject': '[URGENT] ç”Ÿäº§ç¯å¢ƒBugéœ€è¦ç«‹å³ä¿®å¤',
            'from': 'boss@company.com',
            'body': 'å®¢æˆ·æŠ¥å‘Šæ— æ³•ç™»å½•ç³»ç»Ÿï¼Œéœ€è¦é©¬ä¸Šä¿®å¤',
            'received_at': '2025-09-11T18:00:00Z'
        }
        
        # æ£€æµ‹é‡è¦ç¨‹åº¦
        importance_score = gmail.analyze_email_importance(important_email)
        
        assert importance_score >= 8  # é«˜é‡è¦æ€§
        
        # æ£€æµ‹æ˜¯å¦éœ€è¦åˆ›å»ºä»»åŠ¡
        should_create_task = gmail.should_create_task(important_email)
        assert should_create_task == True
        
        # ç”Ÿæˆä»»åŠ¡
        task = gmail.create_task_from_email(important_email)
        assert task.title == "å¤„ç†é‚®ä»¶: [URGENT] ç”Ÿäº§ç¯å¢ƒBugéœ€è¦ç«‹å³ä¿®å¤"
        assert task.priority.level == "critical"
```

### 2. Gité›†æˆæµ‹è¯•

```python
class TestGitIntegration:
    def test_commit_analysis(self):
        """æµ‹è¯•Gitæäº¤åˆ†æ"""
        from integrations.git_integration import GitIntegration
        
        git_integration = GitIntegration()
        
        # æ¨¡æ‹ŸGitæäº¤
        commit_data = {
            'hash': 'abc123def',
            'message': 'å®Œæˆç”¨æˆ·è®¤è¯æ¨¡å—\n\n- å®ç°ç™»å½•åŠŸèƒ½\n- æ·»åŠ å¯†ç åŠ å¯†\n- ä¿®å¤ä¼šè¯ç®¡ç†bug',
            'author': 'developer@example.com',
            'timestamp': '2025-09-11T16:30:00Z',
            'files_changed': ['src/auth.js', 'src/session.js', 'tests/auth.test.js'],
            'lines_added': 125,
            'lines_removed': 18
        }
        
        # åˆ†ææäº¤
        analysis_result = git_integration.analyze_commit(commit_data)
        
        # éªŒè¯åˆ†æç»“æœ
        assert len(analysis_result.extracted_tasks) == 3
        assert "å®ç°ç™»å½•åŠŸèƒ½" in analysis_result.extracted_tasks
        assert "æ·»åŠ å¯†ç åŠ å¯†" in analysis_result.extracted_tasks
        assert "ä¿®å¤ä¼šè¯ç®¡ç†bug" in analysis_result.extracted_tasks
        
        assert analysis_result.work_type == "feature_development"
        assert analysis_result.productivity_score >= 8
    
    def test_project_status_auto_update(self):
        """æµ‹è¯•é¡¹ç›®çŠ¶æ€è‡ªåŠ¨æ›´æ–°"""
        git_integration = GitIntegration()
        
        # æ¨¡æ‹Ÿé¡¹ç›®ç›®å½•
        project_path = "./test_project"
        
        # æ¨¡æ‹Ÿæäº¤è§¦å‘æ›´æ–°
        update_result = git_integration.update_project_status(
            project_path=project_path,
            commit_hash="abc123def",
            trigger_type="post_commit"
        )
        
        # éªŒè¯æ›´æ–°ç»“æœ
        assert update_result.success == True
        assert update_result.status_file_updated == True
        assert "PROJECT_STATUS.md" in update_result.updated_files
```

---

## ğŸ‘¥ ç”¨æˆ·éªŒæ”¶æµ‹è¯•

### 1. ç”¨æˆ·åœºæ™¯æµ‹è¯•

```yaml
# ç”¨æˆ·éªŒæ”¶æµ‹è¯•ç”¨ä¾‹
user_acceptance_tests:
  # åœºæ™¯1: æ¯æ—¥å·¥ä½œè§„åˆ’
  scenario_daily_planning:
    name: "æ¯æ—¥å·¥ä½œè§„åˆ’"
    description: "ç”¨æˆ·æ¯å¤©æ—©ä¸Šä½¿ç”¨ç³»ç»Ÿè§„åˆ’å½“å¤©å·¥ä½œ"
    preconditions:
      - "ç”¨æˆ·æœ‰5ä¸ªæ´»è·ƒé¡¹ç›®"
      - "æœ‰20ä¸ªå¾…å¤„ç†ä»»åŠ¡"
      - "æœ‰æ˜ç¡®çš„ç›®æ ‡è®¾ç½®"
    
    test_steps:
      - step: "ç”¨æˆ·è¾“å…¥'/pm ä»Šå¤©åº”è¯¥åšä»€ä¹ˆ'"
        expected: "ç³»ç»Ÿè¿”å›5-7ä¸ªæ¨èä»»åŠ¡"
        acceptance_criteria:
          - "ä»»åŠ¡æŒ‰ä¼˜å…ˆçº§æ’åº"
          - "åŒ…å«æˆªæ­¢æ—¶é—´ä¿¡æ¯"
          - "æ€»è€—æ—¶ä¸è¶…è¿‡8å°æ—¶"
          - "å“åº”æ—¶é—´<2ç§’"
      
      - step: "ç”¨æˆ·æŸ¥çœ‹ç¬¬ä¸€ä¸ªæ¨èä»»åŠ¡è¯¦æƒ…"
        expected: "æ˜¾ç¤ºä»»åŠ¡è¯¦ç»†ä¿¡æ¯"
        acceptance_criteria:
          - "åŒ…å«é¡¹ç›®å…³è”ä¿¡æ¯"
          - "æ˜¾ç¤ºé¢„è®¡è€—æ—¶"
          - "æ˜¾ç¤ºä¼˜å…ˆçº§ç†ç”±"
      
      - step: "ç”¨æˆ·å¼€å§‹æ‰§è¡Œç¬¬ä¸€ä¸ªä»»åŠ¡"
        expected: "ä»»åŠ¡çŠ¶æ€æ›´æ–°ä¸º'è¿›è¡Œä¸­'"
        acceptance_criteria:
          - "è®°å½•å¼€å§‹æ—¶é—´"
          - "æ›´æ–°é¡¹ç›®çŠ¶æ€"
          - "å…¶ä»–ä»»åŠ¡ä¼˜å…ˆçº§è‡ªåŠ¨è°ƒæ•´"
    
    success_criteria:
      - "ç”¨æˆ·èƒ½åœ¨5åˆ†é’Ÿå†…å®Œæˆå·¥ä½œè§„åˆ’"
      - "æ¨èä»»åŠ¡ä¸ç”¨æˆ·å®é™…éœ€æ±‚åŒ¹é…åº¦>80%"
      - "ç³»ç»Ÿå“åº”æµç•…ï¼Œæ— æ˜æ˜¾å»¶è¿Ÿ"
  
  # åœºæ™¯2: é¡¹ç›®è¿›å±•è·Ÿè¸ª
  scenario_project_tracking:
    name: "é¡¹ç›®è¿›å±•è·Ÿè¸ª"
    description: "ç”¨æˆ·æ£€æŸ¥é¡¹ç›®çŠ¶æ€å’Œè¿›å±•"
    preconditions:
      - "ç”¨æˆ·æ­£åœ¨è¿›è¡Œ2-3ä¸ªé¡¹ç›®"
      - "é¡¹ç›®æœ‰Gitä»“åº“å…³è”"
      - "æœ‰æœ€è¿‘çš„æäº¤è®°å½•"
    
    test_steps:
      - step: "ç”¨æˆ·è¾“å…¥'/pm é¡¹ç›®çŠ¶æ€æ€»è§ˆ'"
        expected: "æ˜¾ç¤ºæ‰€æœ‰é¡¹ç›®çŠ¶æ€"
        acceptance_criteria:
          - "æ˜¾ç¤ºé¡¹ç›®å¥åº·çŠ¶æ€"
          - "æ˜¾ç¤ºå®Œæˆç™¾åˆ†æ¯”"
          - "æ˜¾ç¤ºæœ€åæ›´æ–°æ—¶é—´"
          - "æ ‡è¯†éœ€è¦å…³æ³¨çš„é¡¹ç›®"
      
      - step: "ç”¨æˆ·æŸ¥çœ‹ç‰¹å®šé¡¹ç›®è¯¦æƒ…"
        expected: "æ˜¾ç¤ºé¡¹ç›®è¯¦ç»†çŠ¶æ€"
        acceptance_criteria:
          - "æ˜¾ç¤ºæœ€è¿‘å·¥ä½œæ€»ç»“"
          - "æ˜¾ç¤ºä¸‹ä¸€æ­¥è®¡åˆ’"
          - "æ˜¾ç¤ºé˜»å¡é—®é¢˜"
          - "æ˜¾ç¤ºGitæ´»åŠ¨åˆ†æ"
      
      - step: "ç”¨æˆ·åŸºäºçŠ¶æ€è°ƒæ•´ä¼˜å…ˆçº§"
        expected: "ç³»ç»Ÿæ ¹æ®è°ƒæ•´é‡æ–°è®¡ç®—"
        acceptance_criteria:
          - "ä»»åŠ¡ä¼˜å…ˆçº§å®æ—¶æ›´æ–°"
          - "é¡¹ç›®èµ„æºé‡æ–°åˆ†é…"
          - "æ›´æ–°å·¥ä½œå»ºè®®"
    
    success_criteria:
      - "é¡¹ç›®çŠ¶æ€ä¿¡æ¯å‡†ç¡®åæ˜ å®é™…æƒ…å†µ"
      - "ç”¨æˆ·èƒ½å¿«é€Ÿè¯†åˆ«éœ€è¦å…³æ³¨çš„é—®é¢˜"
      - "çŠ¶æ€æ›´æ–°è§¦å‘åˆç†çš„åç»­å»ºè®®"
```

### 2. å¯ç”¨æ€§æµ‹è¯•è„šæœ¬

```python
# å¯ç”¨æ€§æµ‹è¯•è‡ªåŠ¨åŒ–è„šæœ¬
import pytest
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

class TestUsability:
    def setup_method(self):
        # å¦‚æœæœ‰Webç•Œé¢ï¼Œä½¿ç”¨Seleniumæµ‹è¯•
        # å¯¹äºCLIå·¥å…·ï¼Œä½¿ç”¨pexpectæµ‹è¯•äº¤äº’
        import pexpect
        self.cli_session = None
    
    def test_new_user_onboarding(self):
        """æµ‹è¯•æ–°ç”¨æˆ·ä¸Šæ‰‹æµç¨‹"""
        import pexpect
        
        # å¯åŠ¨PersonalManager CLI
        cli = pexpect.spawn('pm --setup')
        cli.logfile_read = sys.stdout.buffer
        
        # é¢„æœŸçœ‹åˆ°æ¬¢è¿ä¿¡æ¯
        cli.expect('æ¬¢è¿ä½¿ç”¨PersonalManager')
        
        # è¾“å…¥ç”¨æˆ·ä¿¡æ¯
        cli.expect('è¯·è¾“å…¥æ‚¨çš„å§“å:')
        cli.sendline('æµ‹è¯•ç”¨æˆ·')
        
        cli.expect('è¯·è¾“å…¥æ‚¨çš„é‚®ç®±:')
        cli.sendline('test@example.com')
        
        cli.expect('è¯·é€‰æ‹©æ‚¨çš„å·¥ä½œæ—¶é—´:')
        cli.sendline('09:00-18:00')
        
        # éªŒè¯è®¾ç½®å®Œæˆ
        cli.expect('è®¾ç½®å®Œæˆ')
        
        # éªŒè¯é¦–æ¬¡ä½¿ç”¨æŒ‡å¯¼
        cli.sendline('pm help')
        cli.expect('å¸¸ç”¨å‘½ä»¤:')
        
        # æµ‹è¯•åˆ›å»ºç¬¬ä¸€ä¸ªé¡¹ç›®
        cli.sendline('pm create project "æˆ‘çš„ç¬¬ä¸€ä¸ªé¡¹ç›®"')
        cli.expect('é¡¹ç›®åˆ›å»ºæˆåŠŸ')
        
        cli.close()
    
    def test_command_discoverability(self):
        """æµ‹è¯•å‘½ä»¤å¯å‘ç°æ€§"""
        import subprocess
        
        # æµ‹è¯•helpå‘½ä»¤
        result = subprocess.run(['pm', 'help'], capture_output=True, text=True)
        assert result.returncode == 0
        assert 'å¸¸ç”¨å‘½ä»¤' in result.stdout
        assert 'pm today' in result.stdout
        assert 'pm projects' in result.stdout
        
        # æµ‹è¯•å‘½ä»¤è‡ªåŠ¨è¡¥å…¨æç¤º
        result = subprocess.run(['pm', 'proj'], capture_output=True, text=True)
        # åº”è¯¥æç¤ºå¯èƒ½çš„å‘½ä»¤
        assert 'æ‚¨æ˜¯å¦æƒ³è¦æ‰§è¡Œ:' in result.stdout
        assert 'pm projects' in result.stdout
    
    def test_error_message_clarity(self):
        """æµ‹è¯•é”™è¯¯ä¿¡æ¯æ¸…æ™°åº¦"""
        import subprocess
        
        # æµ‹è¯•æ— æ•ˆå‘½ä»¤
        result = subprocess.run(['pm', 'invalid_command'], capture_output=True, text=True)
        assert result.returncode != 0
        assert 'æœªçŸ¥å‘½ä»¤' in result.stdout
        assert 'ä½¿ç”¨ pm help' in result.stdout
        
        # æµ‹è¯•ç¼ºå°‘å‚æ•°
        result = subprocess.run(['pm', 'create'], capture_output=True, text=True)
        assert result.returncode != 0
        assert 'ç¼ºå°‘å‚æ•°' in result.stdout
        assert 'æ­£ç¡®æ ¼å¼:' in result.stdout
        
        # æµ‹è¯•æ— æ•ˆé¡¹ç›®ID
        result = subprocess.run(['pm', 'show', 'invalid-id'], capture_output=True, text=True)
        assert result.returncode != 0
        assert 'é¡¹ç›®ä¸å­˜åœ¨' in result.stdout
        assert 'ä½¿ç”¨ pm projects' in result.stdout
```

### 3. ç”¨æˆ·åé¦ˆæ”¶é›†æœºåˆ¶

```python
# ç”¨æˆ·åé¦ˆæµ‹è¯•
class TestUserFeedback:
    def test_satisfaction_measurement(self):
        """æµ‹è¯•ç”¨æˆ·æ»¡æ„åº¦æµ‹é‡"""
        pm = PersonalManager()
        
        # æ¨¡æ‹Ÿç”¨æˆ·å®Œæˆä»»åŠ¡
        task = pm.create_task({"title": "æµ‹è¯•ä»»åŠ¡"})
        pm.start_task(task.id)
        
        # å®Œæˆä»»åŠ¡æ—¶æ”¶é›†åé¦ˆ
        completion_result = pm.complete_task(
            task.id,
            satisfaction_rating=8,
            difficulty_rating=6,
            feedback="æ¨èçš„ä¼˜å…ˆçº§å¾ˆå‡†ç¡®ï¼Œä½†è€—æ—¶ä¼°ç®—åå°‘"
        )
        
        # éªŒè¯åé¦ˆå­˜å‚¨
        assert completion_result.feedback_recorded == True
        
        # éªŒè¯åé¦ˆç”¨äºæ”¹è¿›ç®—æ³•
        feedback_data = pm.get_feedback_analytics()
        assert len(feedback_data.recent_ratings) > 0
        assert feedback_data.avg_satisfaction >= 0
        assert feedback_data.common_issues is not None
    
    def test_usage_analytics(self):
        """æµ‹è¯•ä½¿ç”¨åˆ†æ"""
        pm = PersonalManager()
        
        # æ¨¡æ‹Ÿç”¨æˆ·ä½¿ç”¨æ¨¡å¼
        usage_data = pm.track_user_behavior({
            "commands_used": ["today", "projects", "status"],
            "session_duration": 1800,  # 30åˆ†é’Ÿ
            "features_accessed": ["task_recommendations", "project_overview"],
            "errors_encountered": 0,
            "goals_achieved": ["å®Œæˆæ¯æ—¥è§„åˆ’", "æŸ¥çœ‹é¡¹ç›®è¿›å±•"]
        })
        
        # åˆ†æä½¿ç”¨æ¨¡å¼
        analytics = pm.analyze_usage_patterns()
        
        assert analytics.most_used_features is not None
        assert analytics.user_efficiency_score > 0
        assert analytics.feature_adoption_rate is not None
```

---

## âš¡ æ€§èƒ½æµ‹è¯•æ–¹æ¡ˆ

### 1. å“åº”æ—¶é—´æµ‹è¯•

```python
# æ€§èƒ½æµ‹è¯•
import pytest
import time
from concurrent.futures import ThreadPoolExecutor
import psutil
import statistics

class TestPerformance:
    def test_response_time_benchmarks(self):
        """æµ‹è¯•å“åº”æ—¶é—´åŸºå‡†"""
        pm = PersonalManager()
        
        # åŸºå‡†å“åº”æ—¶é—´è¦æ±‚
        benchmarks = {
            "get_task_recommendations": 2.0,    # 2ç§’
            "calculate_priorities": 1.5,        # 1.5ç§’ 
            "get_project_status": 1.0,          # 1ç§’
            "create_task": 0.5,                 # 0.5ç§’
            "update_task": 0.3,                 # 0.3ç§’
        }
        
        for operation, max_time in benchmarks.items():
            # é¢„çƒ­
            for _ in range(3):
                getattr(pm, operation)()
            
            # æµ‹è¯•10æ¬¡ï¼Œå–å¹³å‡å€¼
            times = []
            for _ in range(10):
                start_time = time.time()
                getattr(pm, operation)()
                end_time = time.time()
                times.append(end_time - start_time)
            
            avg_time = statistics.mean(times)
            p95_time = statistics.quantiles(times, n=20)[18]  # 95th percentile
            
            # éªŒè¯æ€§èƒ½è¦æ±‚
            assert avg_time < max_time, f"{operation} å¹³å‡å“åº”æ—¶é—´ {avg_time:.2f}s è¶…è¿‡é™åˆ¶ {max_time}s"
            assert p95_time < max_time * 1.5, f"{operation} P95å“åº”æ—¶é—´è¿‡é•¿"
    
    def test_concurrent_load(self):
        """æµ‹è¯•å¹¶å‘è´Ÿè½½"""
        pm = PersonalManager()
        
        def simulate_user_session():
            """æ¨¡æ‹Ÿç”¨æˆ·ä¼šè¯"""
            try:
                # å…¸å‹ç”¨æˆ·æ“ä½œåºåˆ—
                pm.get_task_recommendations()
                time.sleep(0.1)
                
                task = pm.create_task({"title": f"å¹¶å‘æµ‹è¯•ä»»åŠ¡ {time.time()}"})
                time.sleep(0.1)
                
                pm.start_task(task.id)
                time.sleep(0.2)
                
                pm.get_project_status()
                return True
            except Exception as e:
                return False
        
        # æ¨¡æ‹Ÿ10ä¸ªå¹¶å‘ç”¨æˆ·
        with ThreadPoolExecutor(max_workers=10) as executor:
            start_time = time.time()
            
            futures = [executor.submit(simulate_user_session) for _ in range(50)]
            results = [future.result() for future in futures]
            
            end_time = time.time()
        
        # éªŒè¯å¹¶å‘æ€§èƒ½
        success_rate = sum(results) / len(results)
        total_time = end_time - start_time
        
        assert success_rate >= 0.95, f"å¹¶å‘æˆåŠŸç‡ {success_rate:.2%} ä½äºè¦æ±‚"
        assert total_time < 30, f"å¹¶å‘æµ‹è¯•è€—æ—¶ {total_time:.1f}s è¿‡é•¿"
    
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        pm = PersonalManager()
        
        # å¤§é‡æ“ä½œæµ‹è¯•å†…å­˜æ³„æ¼
        for i in range(1000):
            project = pm.create_project({"name": f"æµ‹è¯•é¡¹ç›® {i}"})
            task = pm.create_task({
                "title": f"æµ‹è¯•ä»»åŠ¡ {i}",
                "project_id": project.id
            })
            pm.calculate_task_priorities([task.id])
            
            # æ¯100æ¬¡æ£€æŸ¥ä¸€æ¬¡å†…å­˜
            if i % 100 == 0:
                current_memory = process.memory_info().rss / 1024 / 1024
                memory_growth = current_memory - initial_memory
                
                # å†…å­˜å¢é•¿ä¸åº”è¯¥è¶…è¿‡100MB
                assert memory_growth < 100, f"å†…å­˜å¢é•¿ {memory_growth:.1f}MB è¿‡å¤š"
    
    def test_large_dataset_performance(self):
        """æµ‹è¯•å¤§æ•°æ®é›†æ€§èƒ½"""
        pm = PersonalManager()
        
        # åˆ›å»ºå¤§æ•°æ®é›†
        print("åˆ›å»ºå¤§æ•°æ®é›†...")
        projects = []
        tasks = []
        
        # 100ä¸ªé¡¹ç›®
        for i in range(100):
            project = pm.create_project({"name": f"å¤§æ•°æ®é›†é¡¹ç›® {i}"})
            projects.append(project)
        
        # 5000ä¸ªä»»åŠ¡
        for i in range(5000):
            project = projects[i % len(projects)]
            task = pm.create_task({
                "title": f"å¤§æ•°æ®é›†ä»»åŠ¡ {i}",
                "project_id": project.id,
                "deadline": f"2025-{9 + i % 3}-{(i % 28) + 1:02d}T18:00:00Z"
            })
            tasks.append(task)
        
        # æµ‹è¯•å¤§æ•°æ®é›†ä¸‹çš„æ€§èƒ½
        print("æµ‹è¯•ä¼˜å…ˆçº§è®¡ç®—æ€§èƒ½...")
        start_time = time.time()
        
        recommendations = pm.get_task_recommendations({"max_tasks": 20})
        
        end_time = time.time()
        calculation_time = end_time - start_time
        
        # å¤§æ•°æ®é›†ä¸‹ä¼˜å…ˆçº§è®¡ç®—åº”è¯¥åœ¨10ç§’å†…å®Œæˆ
        assert calculation_time < 10, f"å¤§æ•°æ®é›†ä¼˜å…ˆçº§è®¡ç®—è€—æ—¶ {calculation_time:.1f}s è¿‡é•¿"
        assert len(recommendations) == 20
```

### 2. èµ„æºå ç”¨æµ‹è¯•

```python
class TestResourceUsage:
    def test_cpu_utilization(self):
        """æµ‹è¯•CPUåˆ©ç”¨ç‡"""
        import psutil
        import threading
        
        cpu_samples = []
        stop_monitoring = threading.Event()
        
        def monitor_cpu():
            while not stop_monitoring.is_set():
                cpu_samples.append(psutil.cpu_percent(interval=0.1))
        
        # å¯åŠ¨CPUç›‘æ§
        monitor_thread = threading.Thread(target=monitor_cpu)
        monitor_thread.start()
        
        # æ‰§è¡Œé«˜è´Ÿè½½æ“ä½œ
        pm = PersonalManager()
        
        for i in range(100):
            # åˆ›å»ºå¤æ‚çš„ä¼˜å…ˆçº§è®¡ç®—åœºæ™¯
            tasks = []
            for j in range(50):
                task = pm.create_task({
                    "title": f"CPUæµ‹è¯•ä»»åŠ¡ {i}-{j}",
                    "deadline": f"2025-09-{12 + j % 18}T18:00:00Z",
                    "importance": (j % 10) + 1,
                    "urgency": ((j * 3) % 10) + 1
                })
                tasks.append(task.id)
            
            pm.calculate_task_priorities(tasks)
        
        # åœæ­¢ç›‘æ§
        stop_monitoring.set()
        monitor_thread.join()
        
        # åˆ†æCPUä½¿ç”¨ç‡
        avg_cpu = statistics.mean(cpu_samples)
        max_cpu = max(cpu_samples)
        
        # CPUä½¿ç”¨ç‡ä¸åº”è¯¥é•¿æœŸè¶…è¿‡80%
        assert avg_cpu < 80, f"å¹³å‡CPUä½¿ç”¨ç‡ {avg_cpu:.1f}% è¿‡é«˜"
        assert max_cpu < 95, f"å³°å€¼CPUä½¿ç”¨ç‡ {max_cpu:.1f}% è¿‡é«˜"
    
    def test_disk_io_performance(self):
        """æµ‹è¯•ç£ç›˜IOæ€§èƒ½"""
        pm = PersonalManager()
        
        # æµ‹è¯•å¤§é‡æ•°æ®å†™å…¥
        start_time = time.time()
        
        for i in range(1000):
            project = pm.create_project({
                "name": f"IOæµ‹è¯•é¡¹ç›® {i}",
                "description": "è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•ç£ç›˜IOæ€§èƒ½çš„é¡¹ç›®" * 10  # è¾ƒé•¿æè¿°
            })
            
            # åˆ›å»ºé¡¹ç›®çŠ¶æ€æ–‡ä»¶
            pm.update_project_status(
                project.id,
                work_summary=f"å®Œæˆäº†å¤§é‡å·¥ä½œå†…å®¹çš„æè¿°å’Œåˆ†æ" * 20
            )
        
        end_time = time.time()
        io_time = end_time - start_time
        
        # 1000ä¸ªé¡¹ç›®çš„IOæ“ä½œåº”è¯¥åœ¨30ç§’å†…å®Œæˆ
        assert io_time < 30, f"ç£ç›˜IOæ“ä½œè€—æ—¶ {io_time:.1f}s è¿‡é•¿"
```

---

## ğŸ’» CLIç‰¹æ®Šæµ‹è¯•

### 1. å‘½ä»¤è¡Œäº¤äº’æµ‹è¯•

```bash
#!/usr/bin/env bats
# CLIäº¤äº’æµ‹è¯•

setup() {
    export PM_CONFIG_DIR=$(mktemp -d)
    export PM_TEST_MODE=true
}

teardown() {
    rm -rf "$PM_CONFIG_DIR"
}

@test "CLIåº”è¯¥æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯" {
    run pm --version
    [ "$status" -eq 0 ]
    [[ "$output" =~ PersonalManager.*v[0-9]+\.[0-9]+\.[0-9]+ ]]
}

@test "CLIåº”è¯¥æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯" {
    run pm --help
    [ "$status" -eq 0 ]
    [[ "$output" =~ "Usage:" ]]
    [[ "$output" =~ "Commands:" ]]
    [[ "$output" =~ "pm today" ]]
}

@test "CLIåº”è¯¥å¤„ç†ä¸­æ–‡è¾“å…¥" {
    run pm "ä»Šå¤©åº”è¯¥åšä»€ä¹ˆ"
    [ "$status" -eq 0 ]
    [[ "$output" =~ "ä»»åŠ¡æ¨è" ]]
}

@test "CLIåº”è¯¥å¤„ç†ç®¡é“è¾“å…¥" {
    echo "æ˜¾ç¤ºé¡¹ç›®çŠ¶æ€" | pm
    [ "$status" -eq 0 ]
}

@test "CLIåº”è¯¥æ”¯æŒäº¤äº’æ¨¡å¼" {
    # ä½¿ç”¨expectæµ‹è¯•äº¤äº’æ¨¡å¼
    expect << 'EOF'
spawn pm --interactive
expect "PersonalManager>"
send "today\r"
expect "ä»»åŠ¡æ¨è"
send "exit\r"
expect eof
EOF
}

@test "CLIåº”è¯¥æ­£ç¡®å¤„ç†ä¿¡å·" {
    # å¯åŠ¨é•¿æ—¶é—´è¿è¡Œçš„å‘½ä»¤
    pm calculate-all-priorities &
    PID=$!
    
    # å‘é€ä¸­æ–­ä¿¡å·
    sleep 1
    kill -INT $PID
    
    # éªŒè¯ä¼˜é›…é€€å‡º
    wait $PID
    EXIT_CODE=$?
    [ $EXIT_CODE -eq 130 ]  # SIGINTé€€å‡ºç 
}

@test "CLIåº”è¯¥æ”¯æŒè‡ªåŠ¨è¡¥å…¨" {
    # æµ‹è¯•bashè¡¥å…¨è„šæœ¬
    source <(pm completion bash)
    
    # æ¨¡æ‹ŸTabè¡¥å…¨
    COMPREPLY=()
    _pm_completions "pm" "proj" "pm proj"
    
    # åº”è¯¥åŒ…å«projectså‘½ä»¤
    [[ "${COMPREPLY[*]}" =~ "projects" ]]
}
```

### 2. CLIé…ç½®æµ‹è¯•

```python
# CLIé…ç½®æµ‹è¯•
import pytest
import tempfile
import os
from pathlib import Path
import yaml

class TestCLIConfiguration:
    def test_default_config_creation(self):
        """æµ‹è¯•é»˜è®¤é…ç½®åˆ›å»º"""
        with tempfile.TemporaryDirectory() as temp_dir:
            os.environ['PM_CONFIG_DIR'] = temp_dir
            
            from cli.config import ConfigManager
            config_manager = ConfigManager()
            
            # é¦–æ¬¡è¿è¡Œåº”è¯¥åˆ›å»ºé»˜è®¤é…ç½®
            config = config_manager.load_config()
            
            # éªŒè¯é…ç½®æ–‡ä»¶å­˜åœ¨
            config_file = Path(temp_dir) / "pm-config.yaml"
            assert config_file.exists()
            
            # éªŒè¯é»˜è®¤å€¼
            assert config.user_id is not None
            assert config.working_hours.start_time == "09:00"
            assert config.working_hours.end_time == "18:00"
            assert config.priority_weights.urgency_weight == 0.25
    
    def test_config_validation(self):
        """æµ‹è¯•é…ç½®éªŒè¯"""
        from cli.config import ConfigManager
        
        config_manager = ConfigManager()
        
        # æµ‹è¯•æœ‰æ•ˆé…ç½®
        valid_config = {
            "user_id": "test-user-123",
            "working_hours": {
                "start_time": "09:00",
                "end_time": "17:00"
            },
            "priority_weights": {
                "urgency_weight": 0.3,
                "importance_weight": 0.3,
                "effort_weight": 0.2,
                "alignment_weight": 0.2
            }
        }
        
        # åº”è¯¥é€šè¿‡éªŒè¯
        assert config_manager.validate_config(valid_config) == True
        
        # æµ‹è¯•æ— æ•ˆé…ç½® - æƒé‡å’Œä¸ä¸º1
        invalid_config = valid_config.copy()
        invalid_config["priority_weights"]["urgency_weight"] = 0.5
        
        assert config_manager.validate_config(invalid_config) == False
        
        # æµ‹è¯•æ— æ•ˆæ—¶é—´æ ¼å¼
        invalid_time_config = valid_config.copy()
        invalid_time_config["working_hours"]["start_time"] = "25:00"
        
        assert config_manager.validate_config(invalid_time_config) == False
    
    def test_config_migration(self):
        """æµ‹è¯•é…ç½®ç‰ˆæœ¬è¿ç§»"""
        with tempfile.TemporaryDirectory() as temp_dir:
            # åˆ›å»ºæ—§ç‰ˆæœ¬é…ç½®
            old_config_file = Path(temp_dir) / "pm-config.yaml"
            old_config = {
                "version": "0.9.0",
                "user_name": "Test User",  # æ—§å­—æ®µå
                "work_start": "09:00",     # æ—§å­—æ®µå
                "work_end": "17:00"        # æ—§å­—æ®µå
            }
            
            with open(old_config_file, 'w') as f:
                yaml.dump(old_config, f)
            
            os.environ['PM_CONFIG_DIR'] = temp_dir
            
            from cli.config import ConfigManager
            config_manager = ConfigManager()
            
            # åŠ è½½é…ç½®åº”è¯¥è‡ªåŠ¨è¿ç§»
            config = config_manager.load_config()
            
            # éªŒè¯è¿ç§»ç»“æœ
            assert config.version == "1.0.0"
            assert hasattr(config, 'user_id')
            assert hasattr(config.working_hours, 'start_time')
            assert config.working_hours.start_time == "09:00"
```

### 3. CLIè¾“å‡ºæ ¼å¼æµ‹è¯•

```python
class TestCLIOutput:
    def test_output_formatting(self):
        """æµ‹è¯•è¾“å‡ºæ ¼å¼"""
        from cli.formatter import OutputFormatter
        
        formatter = OutputFormatter()
        
        # æµ‹è¯•ä»»åŠ¡åˆ—è¡¨æ ¼å¼åŒ–
        tasks = [
            {
                "id": "task-1",
                "title": "å®Œæˆç”¨æˆ·è®¤è¯åŠŸèƒ½",
                "priority_score": 85.5,
                "deadline": "2025-09-12T18:00:00Z",
                "project_name": "ä¸ªäººç½‘ç«™"
            },
            {
                "id": "task-2", 
                "title": "ä¿®å¤Safariå…¼å®¹é—®é¢˜",
                "priority_score": 92.1,
                "deadline": "2025-09-11T20:00:00Z",
                "project_name": "ä¸ªäººç½‘ç«™"
            }
        ]
        
        # æµ‹è¯•è¡¨æ ¼æ ¼å¼
        table_output = formatter.format_task_list(tasks, format="table")
        
        assert "ä¼˜å…ˆçº§" in table_output
        assert "é¡¹ç›®" in table_output
        assert "æˆªæ­¢æ—¶é—´" in table_output
        assert "92.1" in table_output  # æœ€é«˜ä¼˜å…ˆçº§ä»»åŠ¡
        
        # æµ‹è¯•JSONæ ¼å¼
        json_output = formatter.format_task_list(tasks, format="json")
        import json
        parsed_json = json.loads(json_output)
        
        assert len(parsed_json) == 2
        assert parsed_json[0]["priority_score"] == 92.1  # åº”è¯¥æŒ‰ä¼˜å…ˆçº§æ’åº
        
        # æµ‹è¯•ç®€æ´æ ¼å¼
        compact_output = formatter.format_task_list(tasks, format="compact")
        
        assert "1." in compact_output  # ç¼–å·æ ¼å¼
        assert "ä¿®å¤Safariå…¼å®¹é—®é¢˜" in compact_output
        assert len(compact_output.split('\n')) <= 4  # ç®€æ´æ ¼å¼è¡Œæ•°é™åˆ¶
    
    def test_color_output(self):
        """æµ‹è¯•å½©è‰²è¾“å‡º"""
        from cli.formatter import ColorFormatter
        
        formatter = ColorFormatter()
        
        # æµ‹è¯•ä¼˜å…ˆçº§é¢œè‰²
        high_priority_text = formatter.colorize_priority("é«˜ä¼˜å…ˆçº§", "critical")
        assert "\033[31m" in high_priority_text  # çº¢è‰²
        
        medium_priority_text = formatter.colorize_priority("ä¸­ä¼˜å…ˆçº§", "medium")  
        assert "\033[33m" in medium_priority_text  # é»„è‰²
        
        # æµ‹è¯•çŠ¶æ€é¢œè‰²
        completed_text = formatter.colorize_status("å·²å®Œæˆ", "completed")
        assert "\033[32m" in completed_text  # ç»¿è‰²
        
        in_progress_text = formatter.colorize_status("è¿›è¡Œä¸­", "in_progress")
        assert "\033[34m" in in_progress_text  # è“è‰²
    
    def test_progress_indicators(self):
        """æµ‹è¯•è¿›åº¦æŒ‡ç¤ºå™¨"""
        from cli.progress import ProgressIndicator
        
        progress = ProgressIndicator()
        
        # æµ‹è¯•è¿›åº¦æ¡
        progress_bar_50 = progress.create_progress_bar(50, 100)
        assert "â–ˆ" in progress_bar_50
        assert "50%" in progress_bar_50
        
        # æµ‹è¯•æ—‹è½¬æŒ‡ç¤ºå™¨
        spinner_frames = [progress.get_spinner_frame(i) for i in range(4)]
        assert len(set(spinner_frames)) > 1  # åº”è¯¥æœ‰ä¸åŒçš„å¸§
```

---

## ğŸ¤– è‡ªåŠ¨åŒ–æµ‹è¯•æ¶æ„

### 1. æŒç»­é›†æˆæµ‹è¯•æµæ°´çº¿

```yaml
# .github/workflows/test.yml
name: PersonalManager Test Suite

on: [push, pull_request]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, '3.10', 3.11]
        node-version: [16, 18, 20]
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: ${{ matrix.node-version }}
      
      - name: Install Python dependencies
        run: |
          pip install -r requirements-dev.txt
          pip install -e .
      
      - name: Install Node.js dependencies
        run: npm ci
      
      - name: Run Python unit tests
        run: |
          pytest tests/unit/ -v --cov=src --cov-report=xml
          
      - name: Run JavaScript unit tests
        run: |
          npm test -- --coverage --watchAll=false
      
      - name: Run CLI tests
        run: |
          bats tests/cli/
      
      - name: Upload coverage reports
        uses: codecov/codecov-action@v3

  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      redis:
        image: redis:6
        ports:
          - 6379:6379
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up test environment
        run: |
          ./scripts/setup-test-env.sh
          
      - name: Run integration tests
        run: |
          pytest tests/integration/ -v --maxfail=3
          
      - name: Run API integration tests
        run: |
          pytest tests/api/ -v --mock-external-apis
          
      - name: Clean up test environment
        run: |
          ./scripts/cleanup-test-env.sh

  performance-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up performance test environment
        run: |
          ./scripts/setup-perf-env.sh
          
      - name: Run performance benchmark
        run: |
          python tests/performance/benchmark.py --output benchmark-results.json
          
      - name: Compare with baseline
        run: |
          python tests/performance/compare.py baseline.json benchmark-results.json
          
      - name: Upload performance results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: benchmark-results.json

  user-acceptance-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    if: github.event_name == 'push' && contains(github.ref, 'release')
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up UAT environment
        run: |
          ./scripts/setup-uat-env.sh
          
      - name: Run user acceptance tests
        run: |
          python tests/acceptance/run_scenarios.py --all
          
      - name: Generate test report
        run: |
          python tests/acceptance/generate_report.py
          
      - name: Upload test report
        uses: actions/upload-artifact@v3
        with:
          name: uat-report
          path: uat-report.html
```

### 2. æµ‹è¯•æ•°æ®ç®¡ç†

```python
# æµ‹è¯•æ•°æ®å·¥å‚
from dataclasses import dataclass
from typing import List, Dict, Any
import uuid
from datetime import datetime, timedelta
import random

class TestDataFactory:
    """æµ‹è¯•æ•°æ®å·¥å‚ç±»"""
    
    @staticmethod
    def create_user(overrides: Dict[str, Any] = None) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•ç”¨æˆ·æ•°æ®"""
        default_user = {
            "id": str(uuid.uuid4()),
            "name": "æµ‹è¯•ç”¨æˆ·",
            "email": "test@example.com",
            "timezone": "Asia/Shanghai",
            "created_at": datetime.now().isoformat(),
            "working_hours": {
                "start_time": "09:00",
                "end_time": "18:00"
            },
            "preferences": {
                "notification_enabled": True,
                "priority_algorithm": "default"
            }
        }
        
        if overrides:
            default_user.update(overrides)
            
        return default_user
    
    @staticmethod
    def create_project(overrides: Dict[str, Any] = None) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•é¡¹ç›®æ•°æ®"""
        project_names = [
            "ä¸ªäººç½‘ç«™é‡æ„", "æœºå™¨å­¦ä¹ é¡¹ç›®", "ç§»åŠ¨åº”ç”¨å¼€å‘",
            "æ•°æ®åˆ†æç³»ç»Ÿ", "åšå®¢å¹³å°", "åœ¨çº¿å•†åº—"
        ]
        
        default_project = {
            "id": str(uuid.uuid4()),
            "name": random.choice(project_names),
            "description": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é¡¹ç›®çš„è¯¦ç»†æè¿°",
            "status": "active",
            "health": "good",
            "dates": {
                "started_at": (datetime.now() - timedelta(days=random.randint(1, 30))).strftime("%Y-%m-%d"),
                "deadline": (datetime.now() + timedelta(days=random.randint(7, 90))).strftime("%Y-%m-%d")
            },
            "progress": {
                "completion_percentage": random.randint(10, 90),
                "milestones_completed": random.randint(1, 5),
                "milestones_total": random.randint(3, 8)
            },
            "priority": {
                "level": random.choice(["high", "medium", "low"]),
                "score": random.randint(30, 95)
            }
        }
        
        if overrides:
            default_project.update(overrides)
            
        return default_project
    
    @staticmethod
    def create_task(project_id: str = None, overrides: Dict[str, Any] = None) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•ä»»åŠ¡æ•°æ®"""
        task_titles = [
            "å®Œæˆç”¨æˆ·è®¤è¯åŠŸèƒ½", "ä¿®å¤Safariå…¼å®¹é—®é¢˜", "ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢",
            "ç¼–å†™APIæ–‡æ¡£", "å®ç°æœç´¢åŠŸèƒ½", "æ·»åŠ å•å…ƒæµ‹è¯•",
            "è®¾è®¡ç”¨æˆ·ç•Œé¢", "éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ", "æ€§èƒ½ä¼˜åŒ–"
        ]
        
        default_task = {
            "id": str(uuid.uuid4()),
            "title": random.choice(task_titles),
            "description": "è¿™æ˜¯ä¸€ä¸ªè¯¦ç»†çš„ä»»åŠ¡æè¿°",
            "status": random.choice(["todo", "in_progress", "completed"]),
            "classification": {
                "project_id": project_id or str(uuid.uuid4()),
                "category": random.choice(["development", "testing", "documentation", "deployment"]),
                "tags": random.sample(["urgent", "feature", "bug", "enhancement"], k=2),
                "context": random.choice(["deep_work", "meeting", "communication"])
            },
            "priority": {
                "level": random.choice(["critical", "high", "medium", "low"]),
                "score": random.randint(20, 100),
                "deadline": (datetime.now() + timedelta(days=random.randint(1, 14))).isoformat(),
                "importance": random.randint(1, 10),
                "urgency": random.randint(1, 10)
            },
            "effort_estimation": {
                "estimated_duration": random.randint(30, 480),  # 30åˆ†é’Ÿåˆ°8å°æ—¶
                "complexity": random.choice(["trivial", "easy", "moderate", "hard"]),
                "energy_required": random.choice(["low", "medium", "high"])
            }
        }
        
        if overrides:
            default_task.update(overrides)
            
        return default_task
    
    @staticmethod
    def create_realistic_dataset(
        users: int = 1,
        projects: int = 5,
        tasks_per_project: int = 10
    ) -> Dict[str, List[Dict[str, Any]]]:
        """åˆ›å»ºçœŸå®çš„æ•°æ®é›†"""
        
        dataset = {
            "users": [],
            "projects": [],
            "tasks": []
        }
        
        # åˆ›å»ºç”¨æˆ·
        for i in range(users):
            user = TestDataFactory.create_user({
                "name": f"æµ‹è¯•ç”¨æˆ·{i+1}",
                "email": f"user{i+1}@example.com"
            })
            dataset["users"].append(user)
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºé¡¹ç›®
        user_id = dataset["users"][0]["id"] if dataset["users"] else str(uuid.uuid4())
        
        for i in range(projects):
            project = TestDataFactory.create_project({
                "user_id": user_id
            })
            dataset["projects"].append(project)
            
            # ä¸ºæ¯ä¸ªé¡¹ç›®åˆ›å»ºä»»åŠ¡
            for j in range(tasks_per_project):
                task = TestDataFactory.create_task(
                    project_id=project["id"],
                    overrides={"user_id": user_id}
                )
                dataset["tasks"].append(task)
        
        return dataset
    
    @staticmethod
    def create_stress_test_dataset() -> Dict[str, List[Dict[str, Any]]]:
        """åˆ›å»ºå‹åŠ›æµ‹è¯•æ•°æ®é›†"""
        return TestDataFactory.create_realistic_dataset(
            users=10,
            projects=100,
            tasks_per_project=20
        )

# æµ‹è¯•æ•°æ®æŒä¹…åŒ–ç®¡ç†
class TestDataManager:
    """æµ‹è¯•æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, storage_path: str = "/tmp/pm_test_data"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(exist_ok=True)
    
    def save_dataset(self, name: str, dataset: Dict[str, Any]):
        """ä¿å­˜æ•°æ®é›†"""
        file_path = self.storage_path / f"{name}.json"
        with open(file_path, 'w') as f:
            json.dump(dataset, f, indent=2, ensure_ascii=False)
    
    def load_dataset(self, name: str) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é›†"""
        file_path = self.storage_path / f"{name}.json"
        if not file_path.exists():
            raise FileNotFoundError(f"Dataset {name} not found")
        
        with open(file_path, 'r') as f:
            return json.load(f)
    
    def cleanup_all(self):
        """æ¸…ç†æ‰€æœ‰æµ‹è¯•æ•°æ®"""
        import shutil
        if self.storage_path.exists():
            shutil.rmtree(self.storage_path)
```

### 3. æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

```python
# æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
from jinja2 import Template

class TestReportGenerator:
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "./test-reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
    
    def generate_comprehensive_report(self, test_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        
        report_template = '''
<!DOCTYPE html>
<html>
<head>
    <title>PersonalManager æµ‹è¯•æŠ¥å‘Š</title>
    <meta charset="utf-8">
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .header { background: #f4f4f4; padding: 20px; border-radius: 5px; }
        .section { margin: 30px 0; }
        .test-pass { color: green; }
        .test-fail { color: red; }
        .test-skip { color: orange; }
        .metrics { display: grid; grid-template-columns: repeat(4, 1fr); gap: 20px; }
        .metric-card { background: #f9f9f9; padding: 15px; border-radius: 5px; text-align: center; }
        table { width: 100%; border-collapse: collapse; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        .coverage-bar { background: #ddd; height: 20px; border-radius: 10px; overflow: hidden; }
        .coverage-fill { height: 100%; background: linear-gradient(to right, #ff6b6b, #feca57, #48ca5c); }
    </style>
</head>
<body>
    <div class="header">
        <h1>PersonalManager æµ‹è¯•æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {{ timestamp }}</p>
        <p>æµ‹è¯•ç¯å¢ƒ: {{ environment }}</p>
        <p>ç‰ˆæœ¬: {{ version }}</p>
    </div>
    
    <div class="section">
        <h2>æµ‹è¯•æ¦‚è§ˆ</h2>
        <div class="metrics">
            <div class="metric-card">
                <h3>{{ summary.total_tests }}</h3>
                <p>æ€»æµ‹è¯•æ•°</p>
            </div>
            <div class="metric-card">
                <h3 class="test-pass">{{ summary.passed_tests }}</h3>
                <p>é€šè¿‡æµ‹è¯•</p>
            </div>
            <div class="metric-card">
                <h3 class="test-fail">{{ summary.failed_tests }}</h3>
                <p>å¤±è´¥æµ‹è¯•</p>
            </div>
            <div class="metric-card">
                <h3>{{ summary.success_rate }}%</h3>
                <p>æˆåŠŸç‡</p>
            </div>
        </div>
    </div>
    
    <div class="section">
        <h2>ä»£ç è¦†ç›–ç‡</h2>
        {% for module, coverage in coverage_data.items() %}
        <div style="margin: 10px 0;">
            <strong>{{ module }}</strong>
            <div class="coverage-bar">
                <div class="coverage-fill" style="width: {{ coverage.percentage }}%;"></div>
            </div>
            <span>{{ coverage.percentage }}% ({{ coverage.covered }}/{{ coverage.total }} è¡Œ)</span>
        </div>
        {% endfor %}
    </div>
    
    <div class="section">
        <h2>æ€§èƒ½æŒ‡æ ‡</h2>
        <table>
            <tr>
                <th>æµ‹è¯•é¡¹</th>
                <th>å¹³å‡å“åº”æ—¶é—´</th>
                <th>P95å“åº”æ—¶é—´</th>
                <th>æˆåŠŸç‡</th>
                <th>çŠ¶æ€</th>
            </tr>
            {% for perf in performance_tests %}
            <tr>
                <td>{{ perf.name }}</td>
                <td>{{ perf.avg_response_time }}ms</td>
                <td>{{ perf.p95_response_time }}ms</td>
                <td>{{ perf.success_rate }}%</td>
                <td class="{% if perf.status == 'pass' %}test-pass{% else %}test-fail{% endif %}">
                    {{ perf.status }}
                </td>
            </tr>
            {% endfor %}
        </table>
    </div>
    
    <div class="section">
        <h2>å¤±è´¥æµ‹è¯•è¯¦æƒ…</h2>
        {% for failure in failed_tests %}
        <div style="margin: 20px 0; padding: 15px; border-left: 4px solid #ff6b6b; background: #fff5f5;">
            <h4>{{ failure.test_name }}</h4>
            <p><strong>é”™è¯¯ç±»å‹:</strong> {{ failure.error_type }}</p>
            <p><strong>é”™è¯¯ä¿¡æ¯:</strong> {{ failure.error_message }}</p>
            <pre style="background: #f4f4f4; padding: 10px; border-radius: 3px;">{{ failure.stack_trace }}</pre>
        </div>
        {% endfor %}
    </div>
    
    <div class="section">
        <h2>æµ‹è¯•æ‰§è¡Œæ—¶é—´åˆ†æ</h2>
        <table>
            <tr>
                <th>æµ‹è¯•å¥—ä»¶</th>
                <th>æµ‹è¯•æ•°é‡</th>
                <th>æ‰§è¡Œæ—¶é—´</th>
                <th>å¹³å‡æ—¶é—´/æµ‹è¯•</th>
            </tr>
            {% for suite in test_suites %}
            <tr>
                <td>{{ suite.name }}</td>
                <td>{{ suite.test_count }}</td>
                <td>{{ suite.duration }}s</td>
                <td>{{ suite.avg_duration }}s</td>
            </tr>
            {% endfor %}
        </table>
    </div>
</body>
</html>
        '''
        
        template = Template(report_template)
        
        # å‡†å¤‡æ¨¡æ¿æ•°æ®
        template_data = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "environment": test_results.get("environment", "unknown"),
            "version": test_results.get("version", "unknown"),
            "summary": test_results.get("summary", {}),
            "coverage_data": test_results.get("coverage", {}),
            "performance_tests": test_results.get("performance", []),
            "failed_tests": test_results.get("failures", []),
            "test_suites": test_results.get("suites", [])
        }
        
        # ç”ŸæˆæŠ¥å‘Š
        html_content = template.render(**template_data)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / f"test-report-{datetime.now().strftime('%Y%m%d-%H%M%S')}.html"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return str(report_file)
```

---

## ğŸ¯ è´¨é‡ä¿éšœæµç¨‹

### 1. è´¨é‡é—¨ç¦æ ‡å‡†

```yaml
# è´¨é‡é—¨ç¦é…ç½®
quality_gates:
  code_quality:
    # ä»£ç è¦†ç›–ç‡è¦æ±‚
    coverage_threshold:
      unit_tests: 70%      # å•å…ƒæµ‹è¯•è¦†ç›–ç‡ >= 70%
      integration_tests: 85% # é›†æˆæµ‹è¯•è¦†ç›–ç‡ >= 85%
      overall: 80%         # æ€»è¦†ç›–ç‡ >= 80%
    
    # ä»£ç è´¨é‡è¦æ±‚
    code_metrics:
      complexity_max: 10   # åœˆå¤æ‚åº¦ <= 10
      duplication_max: 3%  # é‡å¤ä»£ç  <= 3%
      maintainability_min: 70 # å¯ç»´æŠ¤æ€§æŒ‡æ•° >= 70
    
    # å®‰å…¨æ£€æŸ¥
    security_checks:
      vulnerability_scan: required
      dependency_check: required
      secret_detection: required
  
  test_quality:
    # æµ‹è¯•é€šè¿‡ç‡è¦æ±‚
    success_rates:
      unit_tests: 100%     # å•å…ƒæµ‹è¯• 100% é€šè¿‡
      integration_tests: 95% # é›†æˆæµ‹è¯• >= 95% é€šè¿‡
      performance_tests: 90% # æ€§èƒ½æµ‹è¯• >= 90% é€šè¿‡
      user_tests: 100%     # ç”¨æˆ·æµ‹è¯• 100% é€šè¿‡
    
    # æ€§èƒ½åŸºå‡†è¦æ±‚
    performance_benchmarks:
      response_time_p95: 2000ms  # P95å“åº”æ—¶é—´ <= 2ç§’
      memory_usage_max: 100MB    # å†…å­˜ä½¿ç”¨ <= 100MB
      cpu_usage_avg: 50%         # å¹³å‡CPU <= 50%
      error_rate_max: 1%         # é”™è¯¯ç‡ <= 1%
  
  deployment_readiness:
    # éƒ¨ç½²å°±ç»ªæ£€æŸ¥
    readiness_checks:
      all_tests_pass: required
      security_approval: required
      performance_approval: required
      documentation_complete: required
      migration_tested: required
```

### 2. è‡ªåŠ¨åŒ–è´¨é‡æ£€æŸ¥è„šæœ¬

```bash
#!/bin/bash
# è´¨é‡æ£€æŸ¥è‡ªåŠ¨åŒ–è„šæœ¬

set -e

echo "ğŸš€ PersonalManager è´¨é‡æ£€æŸ¥å¼€å§‹..."

# ç¯å¢ƒå˜é‡
export PM_TEST_MODE=true
export PM_CONFIG_DIR=$(mktemp -d)

# æ¸…ç†å‡½æ•°
cleanup() {
    echo "ğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ..."
    rm -rf "$PM_CONFIG_DIR"
}
trap cleanup EXIT

# 1. ä»£ç è´¨é‡æ£€æŸ¥
echo "ğŸ“Š ä»£ç è´¨é‡æ£€æŸ¥..."

# ä»£ç æ ¼å¼æ£€æŸ¥
echo "  - æ£€æŸ¥ä»£ç æ ¼å¼..."
black --check src/ tests/
flake8 src/ tests/
eslint src/**/*.js

# ä»£ç å¤æ‚åº¦æ£€æŸ¥
echo "  - æ£€æŸ¥ä»£ç å¤æ‚åº¦..."
radon cc src/ --min=B --show-complexity
lizard src/ -l python -l javascript -w

# å®‰å…¨æ£€æŸ¥
echo "  - å®‰å…¨æ‰«æ..."
bandit -r src/
npm audit --audit-level high
safety check

# 2. å•å…ƒæµ‹è¯•æ‰§è¡Œ
echo "ğŸ§ª å•å…ƒæµ‹è¯•æ‰§è¡Œ..."

# Pythonå•å…ƒæµ‹è¯•
echo "  - Pythonå•å…ƒæµ‹è¯•..."
coverage run -m pytest tests/unit/ -v --maxfail=5
python_coverage=$(coverage report | grep TOTAL | awk '{print $4}' | sed 's/%//')

# JavaScriptå•å…ƒæµ‹è¯•
echo "  - JavaScriptå•å…ƒæµ‹è¯•..."
npm test -- --coverage --watchAll=false --passWithNoTests
js_coverage=$(cat coverage/lcov-report/index.html | grep -oP '(?<=<span class="strong">)[0-9.]+(?=%</span>)' | head -1)

# CLIæµ‹è¯•
echo "  - CLIåŠŸèƒ½æµ‹è¯•..."
bats tests/cli/

# 3. é›†æˆæµ‹è¯•æ‰§è¡Œ
echo "ğŸ”— é›†æˆæµ‹è¯•æ‰§è¡Œ..."
pytest tests/integration/ -v --maxfail=3

# 4. æ€§èƒ½æµ‹è¯•æ‰§è¡Œ
echo "âš¡ æ€§èƒ½æµ‹è¯•æ‰§è¡Œ..."
python tests/performance/benchmark.py --quick

# 5. è´¨é‡é—¨ç¦æ£€æŸ¥
echo "ğŸšª è´¨é‡é—¨ç¦æ£€æŸ¥..."

check_quality_gate() {
    local metric_name=$1
    local actual_value=$2
    local threshold=$3
    local comparison=$4  # "ge" for >=, "le" for <=
    
    if [ "$comparison" = "ge" ]; then
        if (( $(echo "$actual_value >= $threshold" | bc -l) )); then
            echo "  âœ… $metric_name: $actual_value (>= $threshold)"
            return 0
        else
            echo "  âŒ $metric_name: $actual_value (éœ€è¦ >= $threshold)"
            return 1
        fi
    elif [ "$comparison" = "le" ]; then
        if (( $(echo "$actual_value <= $threshold" | bc -l) )); then
            echo "  âœ… $metric_name: $actual_value (<= $threshold)"
            return 0
        else
            echo "  âŒ $metric_name: $actual_value (éœ€è¦ <= $threshold)"
            return 1
        fi
    fi
}

# è´¨é‡é—¨ç¦æ ‡å‡†æ£€æŸ¥
quality_gate_passed=true

# ä»£ç è¦†ç›–ç‡æ£€æŸ¥
if ! check_quality_gate "Pythonä»£ç è¦†ç›–ç‡" "$python_coverage" "70" "ge"; then
    quality_gate_passed=false
fi

if ! check_quality_gate "JavaScriptä»£ç è¦†ç›–ç‡" "$js_coverage" "70" "ge"; then
    quality_gate_passed=false
fi

# æµ‹è¯•é€šè¿‡ç‡æ£€æŸ¥ (ä»æµ‹è¯•è¾“å‡ºè§£æ)
unit_test_success_rate=$(pytest tests/unit/ --tb=no -q | grep -oP '\d+(?=% passed)' || echo "100")
if ! check_quality_gate "å•å…ƒæµ‹è¯•é€šè¿‡ç‡" "$unit_test_success_rate" "100" "ge"; then
    quality_gate_passed=false
fi

# æ€§èƒ½åŸºå‡†æ£€æŸ¥
response_time_p95=$(python -c "
import json
with open('benchmark-results.json') as f:
    data = json.load(f)
    print(data.get('response_time_p95', 0))
")

if ! check_quality_gate "P95å“åº”æ—¶é—´" "$response_time_p95" "2000" "le"; then
    quality_gate_passed=false
fi

# 6. ç”Ÿæˆè´¨é‡æŠ¥å‘Š
echo "ğŸ“‹ ç”Ÿæˆè´¨é‡æŠ¥å‘Š..."

cat > quality-report.json << EOF
{
    "timestamp": "$(date -Iseconds)",
    "quality_gate_passed": $quality_gate_passed,
    "metrics": {
        "python_coverage": $python_coverage,
        "js_coverage": $js_coverage,
        "unit_test_success_rate": $unit_test_success_rate,
        "response_time_p95": $response_time_p95
    },
    "test_results": {
        "unit_tests": "$(cat pytest-unit.xml | grep -c 'testcase')",
        "integration_tests": "$(cat pytest-integration.xml | grep -c 'testcase')",
        "cli_tests": "$(find tests/cli -name '*.bats' | wc -l)"
    }
}
EOF

# 7. ç»“æœæ€»ç»“
echo "ğŸ“ˆ è´¨é‡æ£€æŸ¥æ€»ç»“..."

if [ "$quality_gate_passed" = true ]; then
    echo "ğŸ‰ è´¨é‡æ£€æŸ¥é€šè¿‡ï¼å¯ä»¥è¿›è¡Œéƒ¨ç½²ã€‚"
    exit 0
else
    echo "âŒ è´¨é‡æ£€æŸ¥å¤±è´¥ï¼è¯·ä¿®å¤é—®é¢˜åé‡æ–°æäº¤ã€‚"
    exit 1
fi
```

---

**ğŸ“ å¤‡æ³¨**: æœ¬æµ‹è¯•ç­–ç•¥ä¸éªŒè¯æ–¹æ¡ˆä¸ºPersonalManageræä¾›äº†å…¨é¢çš„è´¨é‡ä¿éšœä½“ç³»ï¼Œæ¶µç›–ä»å•å…ƒæµ‹è¯•åˆ°ç”¨æˆ·éªŒæ”¶çš„å„ä¸ªå±‚é¢ï¼Œç¡®ä¿ç³»ç»Ÿçš„å¯é æ€§ã€æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚

**ğŸ”„ åŒæ­¥çŠ¶æ€**: å·²åŒæ­¥åˆ°PersonalManagerå¼€å‘æµç¨‹ (2025-09-11 18:30)